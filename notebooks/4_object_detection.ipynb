{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f95699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Data handling / visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import yaml\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc138c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (11): Concat()\n",
      "      (12): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (14): Concat()\n",
      "      (15): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (17): Concat()\n",
      "      (18): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (20): Concat()\n",
      "      (21): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Setup device\n",
    "# --------------------------\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Load pre-trained YOLOv8 model\n",
    "# --------------------------\n",
    "# Nano YOLOv8 for CPU\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "print(\"Model loaded:\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7db607e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO dataset prepared and voc.yaml created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Dataset setup\n",
    "# --------------------------\n",
    "# Paths\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\")\n",
    "yolo_dataset_dir = os.path.join(repo_root, \"YOLO_VOC\")\n",
    "os.makedirs(yolo_dataset_dir, exist_ok=True)\n",
    "\n",
    "# VOC Classes\n",
    "VOC_CLASSES = [\n",
    "    'aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow',\n",
    "    'diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "\n",
    "# Function to convert bounding boxes\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1.0 / size[0]\n",
    "    dh = 1.0 / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    return x*dw, y*dh, w*dw, h*dh\n",
    "\n",
    "# Conversion function\n",
    "def voc_to_yolo(voc_root, year=\"2012\", split=\"train\", output_dir=\"YOLO_VOC\"):\n",
    "    img_dir = os.path.join(voc_root, f\"VOC{year}\", \"JPEGImages\")\n",
    "    ann_dir = os.path.join(voc_root, f\"VOC{year}\", \"Annotations\")\n",
    "    split_file = os.path.join(voc_root, f\"VOC{year}\", \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "\n",
    "    # Output dirs\n",
    "    txt_output_dir = os.path.join(output_dir, split, \"labels\")\n",
    "    img_output_dir = os.path.join(output_dir, split, \"images\")\n",
    "    os.makedirs(txt_output_dir, exist_ok=True)\n",
    "    os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "    # Read official split list\n",
    "    with open(split_file, \"r\") as f:\n",
    "        img_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for img_id in img_ids:\n",
    "        # Copy image\n",
    "        src_img = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
    "        dst_img = os.path.join(img_output_dir, f\"{img_id}.jpg\")\n",
    "        shutil.copy(src_img, dst_img)\n",
    "\n",
    "        # Convert annotation\n",
    "        xml_file = os.path.join(ann_dir, f\"{img_id}.xml\")\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        w = int(root.find(\"size/width\").text)\n",
    "        h = int(root.find(\"size/height\").text)\n",
    "\n",
    "        yolo_labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            cls_name = obj.find(\"name\").text\n",
    "            if cls_name not in VOC_CLASSES:\n",
    "                continue\n",
    "            cls_id = VOC_CLASSES.index(cls_name)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            x_center, y_center, bw, bh = convert_bbox((w, h), (xmin, xmax, ymin, ymax))\n",
    "            yolo_labels.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\")\n",
    "\n",
    "        txt_file_path = os.path.join(txt_output_dir, f\"{img_id}.txt\")\n",
    "        with open(txt_file_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_labels))\n",
    "\n",
    "\n",
    "# Convert train and val sets\n",
    "voc_to_yolo(voc_root, \"2012\", \"train\", yolo_dataset_dir)\n",
    "voc_to_yolo(voc_root, \"2012\", \"val\", yolo_dataset_dir)\n",
    "\n",
    "# Generate YAML config for YOLOv8\n",
    "voc_yaml = {\n",
    "    'train': os.path.join(yolo_dataset_dir, 'train', 'images'),\n",
    "    'val': os.path.join(yolo_dataset_dir, 'val', 'images'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(repo_root, \"voc.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(voc_yaml, f)\n",
    "print(\"YOLO dataset prepared and voc.yaml created at:\", yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f4bddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train subset created with 800 images.\n",
      "val subset created with 200 images.\n",
      "Subset YAML created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Subset setup\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\", \"VOC2012\")\n",
    "subset_dir = os.path.join(repo_root, \"YOLO_VOC_subset\")\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "subset_sizes = {\n",
    "    \"train\": 800,   # number of images for CPU testing\n",
    "    \"val\": 200\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Function to create subset\n",
    "# --------------------------\n",
    "def create_yolo_subset(voc_root, original_dir, subset_dir, split, num_images):\n",
    "    split_file = os.path.join(voc_root, \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "    orig_images_dir = os.path.join(original_dir, split, \"images\")\n",
    "    orig_labels_dir = os.path.join(original_dir, split, \"labels\")\n",
    "\n",
    "    subset_images_dir = os.path.join(subset_dir, split, \"images\")\n",
    "    subset_labels_dir = os.path.join(subset_dir, split, \"labels\")\n",
    "    os.makedirs(subset_images_dir, exist_ok=True)\n",
    "    os.makedirs(subset_labels_dir, exist_ok=True)\n",
    "\n",
    "    # Load official split IDs\n",
    "    with open(split_file, \"r\") as f:\n",
    "        split_ids = [line.strip() + \".jpg\" for line in f.readlines()]\n",
    "\n",
    "    # Keep only those that exist in original_dir\n",
    "    available_images = [f for f in split_ids if f in os.listdir(orig_images_dir)]\n",
    "\n",
    "    # Random sample\n",
    "    #sampled_images = random.sample(available_images, min(num_images, len(available_images)))\n",
    "    \n",
    "    # Take the first N images instead of random sample\n",
    "    sampled_images = available_images[:num_images]\n",
    "\n",
    "    for img_file in sampled_images:\n",
    "        # Copy image\n",
    "        shutil.copy(os.path.join(orig_images_dir, img_file),\n",
    "                    os.path.join(subset_images_dir, img_file))\n",
    "        # Copy corresponding label\n",
    "        label_file = img_file.replace(\".jpg\", \".txt\")\n",
    "        shutil.copy(os.path.join(orig_labels_dir, label_file),\n",
    "                    os.path.join(subset_labels_dir, label_file))\n",
    "\n",
    "    print(f\"{split} subset created with {len(sampled_images)} images.\")\n",
    "\n",
    "# --------------------------\n",
    "# Create subsets\n",
    "# --------------------------\n",
    "for split in [\"train\", \"val\"]:\n",
    "    create_yolo_subset(voc_root, yolo_dataset_dir, subset_dir, split, subset_sizes[split])\n",
    "\n",
    "# --------------------------\n",
    "# Create subset YAML\n",
    "# --------------------------\n",
    "subset_yaml = {\n",
    "    'train': os.path.join(subset_dir, 'train'),\n",
    "    'val': os.path.join(subset_dir, 'val'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "subset_yaml_path = os.path.join(repo_root, \"voc_subset.yaml\")\n",
    "with open(subset_yaml_path, \"w\") as f:\n",
    "    yaml.dump(subset_yaml, f)\n",
    "print(\"Subset YAML created at:\", subset_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdf68d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.196 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train113333, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train113333, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 70/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 122.546.2 MB/s, size: 114.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels.cache... 3739 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 3739/3739 3735708.1it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 144.150.0 MB/s, size: 78.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache... 1055 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1055/1055 1023829.4it/s 0.0s\n",
      "Plotting labels to runs\\detect\\train113333\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train113333\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G      1.761      2.629       1.55          9        224: 100% ━━━━━━━━━━━━ 935/935 1.6it/s 9:42<0.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.2it/s 59.5s0.4ss\n",
      "                   all       1055       2920      0.222      0.168      0.121     0.0669\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10         0G      1.743      2.647      1.539         19        224: 100% ━━━━━━━━━━━━ 935/935 1.7it/s 9:12<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.4it/s 54.7s0.4ss\n",
      "                   all       1055       2920       0.21      0.187      0.126     0.0688\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10         0G       1.73      2.604      1.535          6        224: 100% ━━━━━━━━━━━━ 935/935 0.33it/s 47:540.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.2it/s 59.7s0.3ss\n",
      "                   all       1055       2920      0.228      0.174       0.13     0.0717\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10         0G      1.735      2.602       1.53         19        224: 100% ━━━━━━━━━━━━ 935/935 1.8it/s 8:47<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.0it/s 1:050.4sss\n",
      "                   all       1055       2920      0.199      0.206      0.126     0.0691\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10         0G      1.712       2.59      1.519         10        224: 100% ━━━━━━━━━━━━ 935/935 0.03it/s 9:12:430.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 0.06it/s 34:47.5ss\n",
      "                   all       1055       2920      0.208      0.189      0.126     0.0682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10         0G      1.708      2.557      1.506          7        224: 100% ━━━━━━━━━━━━ 935/935 2.1it/s 7:23<0.4s9\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.6it/s 51.7s0.4ss\n",
      "                   all       1055       2920      0.245       0.19      0.139     0.0786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10         0G      1.706      2.543      1.511         11        224: 100% ━━━━━━━━━━━━ 935/935 2.1it/s 7:23<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.4it/s 55.0s0.4ss\n",
      "                   all       1055       2920      0.236      0.198      0.138     0.0782\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10         0G      1.706      2.545      1.514         10        224: 100% ━━━━━━━━━━━━ 935/935 2.0it/s 7:39<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.6it/s 51.6s0.4ss\n",
      "                   all       1055       2920      0.215      0.204      0.143     0.0805\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10         0G      1.714      2.531      1.507          3        224: 100% ━━━━━━━━━━━━ 935/935 2.0it/s 7:41<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.2it/s 1:010.3sss\n",
      "                   all       1055       2920      0.232      0.207      0.142     0.0806\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10         0G       1.71      2.521      1.496          8        224: 100% ━━━━━━━━━━━━ 935/935 1.6it/s 9:52<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.2it/s 1:010.4sss\n",
      "                   all       1055       2920      0.259      0.195      0.147     0.0831\n",
      "\n",
      "10 epochs completed in 11.865 hours.\n",
      "Optimizer stripped from runs\\detect\\train113333\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train113333\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train113333\\weights\\best.pt...\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 132/132 2.1it/s 1:020.4sss\n",
      "                   all       1055       2920      0.257      0.196      0.147     0.0831\n",
      "             aeroplane         41         77      0.346      0.247      0.228      0.142\n",
      "               bicycle         43         57      0.182      0.193     0.0667     0.0379\n",
      "                  bird         59        118      0.156     0.0932      0.063     0.0364\n",
      "                  boat         38         58      0.272     0.0345     0.0282     0.0118\n",
      "                bottle         65        137      0.305     0.0353     0.0424     0.0187\n",
      "                   bus         39         52      0.227      0.519      0.343      0.227\n",
      "                   car         95        213      0.373      0.146      0.129     0.0772\n",
      "                   cat         83         94      0.318      0.303      0.284      0.157\n",
      "                 chair        115        270      0.225     0.0333     0.0502     0.0194\n",
      "                   cow         25         55     0.0829     0.0545     0.0377     0.0233\n",
      "           diningtable         51         54      0.247      0.259      0.167     0.0764\n",
      "                   dog        117        140      0.235      0.246      0.141      0.082\n",
      "                 horse         38         71      0.248      0.242      0.176      0.117\n",
      "             motorbike         45         61      0.216       0.23      0.185      0.119\n",
      "                person        483       1077      0.454      0.359      0.351      0.178\n",
      "           pottedplant         50        112      0.219    0.00893     0.0168    0.00923\n",
      "                 sheep         28         85      0.273      0.118     0.0886     0.0393\n",
      "                  sofa         63         76      0.287     0.0921      0.107     0.0612\n",
      "                 train         47         50      0.185       0.42      0.222      0.137\n",
      "             tvmonitor         49         63       0.28      0.278      0.208     0.0909\n",
      "Speed: 0.5ms preprocess, 46.2ms inference, 0.0ms loss, 2.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train113333\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Fine-tune model (CPU-friendly)\n",
    "# --------------------------\n",
    "results = model.train(\n",
    "    data=subset_yaml_path,      #yaml_path for full dataset; subset_yaml_path for debugging/CPU\n",
    "    epochs=10,            # lower for CPU\n",
    "    batch=4,             # small batch for CPU\n",
    "    imgsz=224,           # smaller image size speeds up CPU training\n",
    "    device=device,        # CPU\n",
    "    freeze=10           #freezes first 10 layers (backbone)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d6981a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 702.0458.3 MB/s, size: 274.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache... 1055 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1055/1055 516395.2it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 264/264 8.7it/s 30.5s<0.1ss\n",
      "                   all       1055       2920        0.2      0.208      0.155     0.0812\n",
      "             aeroplane         41         77      0.205      0.221      0.223      0.121\n",
      "               bicycle         43         57      0.107      0.263      0.136     0.0724\n",
      "                  bird         59        118        0.1      0.119     0.0651     0.0281\n",
      "                  boat         38         58      0.462     0.0598     0.0952     0.0343\n",
      "                bottle         65        137      0.211     0.0292     0.0835     0.0331\n",
      "                   bus         39         52      0.163      0.538      0.342      0.209\n",
      "                   car         95        213      0.199      0.155      0.154     0.0983\n",
      "                   cat         83         94      0.199      0.319      0.191     0.0918\n",
      "                 chair        115        270      0.124     0.0111     0.0623     0.0324\n",
      "                   cow         25         55      0.129     0.0727     0.0899      0.055\n",
      "           diningtable         51         54       0.22      0.241      0.155     0.0613\n",
      "                   dog        117        140      0.199      0.307      0.159     0.0903\n",
      "                 horse         38         71      0.132      0.282      0.129     0.0826\n",
      "             motorbike         45         61      0.151      0.361      0.207      0.105\n",
      "                person        483       1077      0.489      0.384      0.397      0.197\n",
      "           pottedplant         50        112      0.106    0.00893     0.0585     0.0306\n",
      "                 sheep         28         85      0.248     0.0941     0.0922     0.0629\n",
      "                  sofa         63         76      0.165     0.0395     0.0783      0.028\n",
      "                 train         47         50      0.143       0.44      0.172     0.0867\n",
      "             tvmonitor         49         63      0.257      0.206      0.212      0.105\n",
      "Speed: 0.4ms preprocess, 24.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train1133332\u001b[0m\n",
      "Overall mAP@0.5:0.95: 0.08123813063649525\n",
      "Overall Precision: 0.20043111279101905\n",
      "Overall Recall: 0.2075722942867008\n",
      "\n",
      "Per-class mAP:\n",
      "                class  mAP@0.5:0.95\n",
      "0          aeroplane      0.120976\n",
      "1            bicycle      0.072351\n",
      "2               bird      0.028068\n",
      "3               boat      0.034316\n",
      "4             bottle      0.033132\n",
      "5                bus      0.208784\n",
      "6                car      0.098295\n",
      "7                cat      0.091793\n",
      "8              chair      0.032429\n",
      "9                cow      0.055023\n",
      "10       diningtable      0.061320\n",
      "11               dog      0.090323\n",
      "12             horse      0.082623\n",
      "13         motorbike      0.105190\n",
      "14            person      0.196873\n",
      "15       pottedplant      0.030556\n",
      "16             sheep      0.062879\n",
      "17              sofa      0.027980\n",
      "18             train      0.086742\n",
      "19         tvmonitor      0.105109\n",
      "Overall      Overall      0.081238\n",
      "\n",
      "Per-class Precision:\n",
      "                class  Precision\n",
      "0          aeroplane   0.204716\n",
      "1            bicycle   0.106688\n",
      "2               bird   0.100448\n",
      "3               boat   0.461985\n",
      "4             bottle   0.210527\n",
      "5                bus   0.163027\n",
      "6                car   0.198749\n",
      "7                cat   0.198501\n",
      "8              chair   0.124312\n",
      "9                cow   0.129013\n",
      "10       diningtable   0.219642\n",
      "11               dog   0.199314\n",
      "12             horse   0.131787\n",
      "13         motorbike   0.151345\n",
      "14            person   0.489324\n",
      "15       pottedplant   0.106412\n",
      "16             sheep   0.247825\n",
      "17              sofa   0.164872\n",
      "18             train   0.143172\n",
      "19         tvmonitor   0.256963\n",
      "Overall      Overall   0.200431\n",
      "\n",
      "Per-class Recall:\n",
      "                class    Recall\n",
      "0          aeroplane  0.220779\n",
      "1            bicycle  0.263158\n",
      "2               bird  0.118644\n",
      "3               boat  0.059789\n",
      "4             bottle  0.029197\n",
      "5                bus  0.538462\n",
      "6                car  0.154930\n",
      "7                cat  0.319149\n",
      "8              chair  0.011111\n",
      "9                cow  0.072727\n",
      "10       diningtable  0.240741\n",
      "11               dog  0.307143\n",
      "12             horse  0.281690\n",
      "13         motorbike  0.360656\n",
      "14            person  0.384401\n",
      "15       pottedplant  0.008929\n",
      "16             sheep  0.094118\n",
      "17              sofa  0.039474\n",
      "18             train  0.440000\n",
      "19         tvmonitor  0.206349\n",
      "Overall      Overall  0.207572\n",
      "\n",
      "Per-class F1:\n",
      "           class        F1\n",
      "0     aeroplane  0.212444\n",
      "1       bicycle  0.151824\n",
      "2          bird  0.108790\n",
      "3          boat  0.105876\n",
      "4        bottle  0.051282\n",
      "5           bus  0.250278\n",
      "6           car  0.174124\n",
      "7           cat  0.244765\n",
      "8         chair  0.020399\n",
      "9           cow  0.093018\n",
      "10  diningtable  0.229707\n",
      "11          dog  0.241749\n",
      "12        horse  0.179565\n",
      "13    motorbike  0.213216\n",
      "14       person  0.430562\n",
      "15  pottedplant  0.016475\n",
      "16        sheep  0.136424\n",
      "17         sofa  0.063697\n",
      "18        train  0.216045\n",
      "19    tvmonitor  0.228891\n",
      "20      Overall  0.203939\n",
      "All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Compute full validation metrics\n",
    "# --------------------------\n",
    "metrics = model.val(\n",
    "    data=subset_yaml_path,\n",
    "    split=\"val\",\n",
    "    imgsz=320,\n",
    "    conf=0.1\n",
    ")\n",
    "\n",
    "print(\"Overall mAP@0.5:0.95:\", metrics.box.map)\n",
    "print(\"Overall Precision:\", metrics.box.mp)\n",
    "print(\"Overall Recall:\", metrics.box.mr)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 6. Export per-class metrics into DataFrames\n",
    "# --------------------------\n",
    "\n",
    "# mAP DataFrame\n",
    "df_map = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"mAP@0.5:0.95\": metrics.box.maps\n",
    "})\n",
    "df_map.loc[\"Overall\"] = [\"Overall\", metrics.box.map]\n",
    "\n",
    "# Precision DataFrame\n",
    "df_precision = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Precision\": metrics.box.p\n",
    "})\n",
    "df_precision.loc[\"Overall\"] = [\"Overall\", metrics.box.mp]\n",
    "\n",
    "# Recall DataFrame\n",
    "df_recall = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Recall\": metrics.box.r\n",
    "})\n",
    "df_recall.loc[\"Overall\"] = [\"Overall\", metrics.box.mr]\n",
    "\n",
    "# F1 DataFrame\n",
    "f1_per_class = 2 * (metrics.box.p * metrics.box.r) / (metrics.box.p + metrics.box.r + 1e-6)\n",
    "overall_f1 = 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr + 1e-6)\n",
    "df_f1 = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"F1\": f1_per_class\n",
    "})\n",
    "df_f1.loc[len(df_f1)] = [\"Overall\", overall_f1]\n",
    "\n",
    "print(\"\\nPer-class mAP:\\n\", df_map)\n",
    "print(\"\\nPer-class Precision:\\n\", df_precision)\n",
    "print(\"\\nPer-class Recall:\\n\", df_recall)\n",
    "print(\"\\nPer-class F1:\\n\", df_f1)\n",
    "\n",
    "# Save to CSVs\n",
    "df_map.to_csv(os.path.join(repo_root, \"yolo_val_map.csv\"), index=False)\n",
    "df_precision.to_csv(os.path.join(repo_root, \"yolo_val_precision.csv\"), index=False)\n",
    "df_recall.to_csv(os.path.join(repo_root, \"yolo_val_recall.csv\"), index=False)\n",
    "df_f1.to_csv(os.path.join(repo_root, \"yolo_val_f1.csv\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 7. Combined grouped bar chart (Precision, Recall, F1)\n",
    "# --------------------------\n",
    "classes = df_precision[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))  # positions\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width, df_precision[\"Precision\"][:-1], width=width, label=\"Precision\")\n",
    "plt.bar(x, df_recall[\"Recall\"][:-1], width=width, label=\"Recall\")\n",
    "plt.bar(x + width, df_f1[\"F1\"][:-1], width=width, label=\"F1\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Precision, Recall, and F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_prf1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cf7f5359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr 0.8432202442902987\n",
      "pval 3.031680198602067e-06\n",
      "Generated scatterplot: Per-class mAP vs F1 (with correlation)\n"
     ]
    }
   ],
   "source": [
    "# Exclude \"Overall\" row\n",
    "perclass_map = df_map[\"mAP@0.5:0.95\"][:-1]\n",
    "perclass_f1 = df_f1[\"F1\"][:-1]\n",
    "classes = df_map[\"class\"][:-1]\n",
    "\n",
    "# Compute correlation\n",
    "corr, pval = pearsonr(perclass_map, perclass_f1)\n",
    "print(\"corr\", corr)\n",
    "print(\"pval\", pval)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter with bigger dots and specific color\n",
    "sns.scatterplot(\n",
    "    x=perclass_map,\n",
    "    y=perclass_f1,\n",
    "    s=120,                # bigger dots\n",
    "    color=\"#009E73\"       # custom color\n",
    ")\n",
    "\n",
    "# Annotate each class with larger, colored labels\n",
    "for i, cls in enumerate(classes):\n",
    "    plt.text(\n",
    "        perclass_map.iloc[i] + 0.005,\n",
    "        perclass_f1.iloc[i] + 0.005,\n",
    "        cls,\n",
    "        fontsize=8,        # larger font\n",
    "        color=\"#009E73\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"mAP@0.5:0.95\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(f\"YOLOv8 Per-Class mAP vs F1 (Validation)\\nPearson r={corr:.2f}, p={pval:.3f}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_map_vs_f1_scatter.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated scatterplot: Per-class mAP vs F1 (with correlation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7f972399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated grouped bar chart: Per-class mAP vs F1\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Compare per-class mAP vs F1\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "classes = df_map[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35  # bar width\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width/2, df_map[\"mAP@0.5:0.95\"][:-1], width=width, label=\"mAP@0.5:0.95\")\n",
    "plt.bar(x + width/2, df_f1[\"F1\"][:-1], width=width, label=\"F1 Score\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Comparison: mAP vs F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_map_vs_f1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated grouped bar chart: Per-class mAP vs F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4910bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_01.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_02.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_03.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_04.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_05.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_06.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_07.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_08.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_09.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_10.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Side-by-side GT vs Predictions (10 random val images)\n",
    "# --------------------------\n",
    "\n",
    "val_images_dir = os.path.join(subset_dir, \"val\", \"images\")\n",
    "val_labels_dir = os.path.join(subset_dir, \"val\", \"labels\")\n",
    "\n",
    "# Randomly select 10 val images\n",
    "all_val_images = glob.glob(os.path.join(val_images_dir, \"*.jpg\"))\n",
    "sample_val_images = random.sample(all_val_images, min(10, len(all_val_images)))\n",
    "\n",
    "comparison_dir = os.path.join(repo_root, \"comparison_predictions\")\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "for idx, img_path in enumerate(sample_val_images, start=1):\n",
    "    # Load GT labels\n",
    "    label_path = os.path.join(val_labels_dir, os.path.basename(img_path).replace(\".jpg\", \".txt\"))\n",
    "    gt_boxes = []\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                cls_id, x, y, w, h = line.strip().split()\n",
    "                cls_id = int(cls_id)\n",
    "                gt_boxes.append((VOC_CLASSES[cls_id], float(x), float(y), float(w), float(h)))\n",
    "\n",
    "    # Run YOLO prediction\n",
    "    results = model.predict(source=img_path, imgsz=320, conf=0.1, verbose=False)\n",
    "    res = results[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Left: Ground truth (draw bounding boxes manually)\n",
    "    img = mpimg.imread(img_path)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    h, w = img.shape[:2]\n",
    "    for cls_name, x, y, bw, bh in gt_boxes:\n",
    "        xmin = int((x - bw/2) * w)\n",
    "        ymin = int((y - bh/2) * h)\n",
    "        xmax = int((x + bw/2) * w)\n",
    "        ymax = int((y + bh/2) * h)\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin,\n",
    "                             linewidth=2, edgecolor=\"lime\", facecolor=\"none\")\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].text(xmin, ymin-5, cls_name, color=\"lime\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "    # Right: YOLO predictions (ultralytics res.plot())\n",
    "    img_pred = res.plot()\n",
    "    axes[1].imshow(img_pred)\n",
    "    axes[1].set_title(\"YOLO Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_dir, f\"comparison_{idx:02d}.png\")\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved GT vs Prediction comparison: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54ac0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: (21, 2)\n",
      "       class  Precision    Recall        F1\n",
      "0  aeroplane   0.500000  0.250000  0.333333\n",
      "1    bicycle   1.000000  0.250000  0.400000\n",
      "2       bird   0.800000  0.500000  0.615385\n",
      "3        car   0.600000  0.315789  0.413793\n",
      "4        cat   0.916667  0.687500  0.785714\n",
      "5      chair   0.538462  0.411765  0.466667\n",
      "6      horse   1.000000  0.250000  0.400000\n",
      "7  motorbike   1.000000  0.125000  0.222222\n",
      "8     person   0.835821  0.910569  0.871595\n",
      "9  tvmonitor   0.500000  0.571429  0.533333\n",
      "       class  Precision    Recall        F1\n",
      "0  aeroplane   0.000000  0.000000  0.000000\n",
      "1    bicycle   0.000000  0.000000  0.000000\n",
      "2       bird   0.000000  0.000000  0.000000\n",
      "3        car   0.062731  0.093407  0.075055\n",
      "4        cat   0.050725  0.093333  0.065727\n",
      "5      chair   0.000000  0.000000  0.000000\n",
      "6      horse   0.000000  0.000000  0.000000\n",
      "7  motorbike   0.000000  0.000000  0.000000\n",
      "8     person   0.186667  0.245077  0.211920\n",
      "9  tvmonitor   0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Comparing YOLO and ResNet's classification performance on Pascal VOC\n",
    "# --------------------------\n",
    "# \n",
    "# # --------------------------\n",
    "# Paths to CSV files\n",
    "# --------------------------\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "\n",
    "resnet_csv = os.path.join(repo_root, \"resnet_val_precision_recall_f1.csv\")\n",
    "yolo_precision_csv = os.path.join(repo_root, \"yolo_val_precision.csv\")\n",
    "yolo_recall_csv = os.path.join(repo_root, \"yolo_val_recall.csv\")\n",
    "yolo_f1_csv = os.path.join(repo_root, \"yolo_val_f1.csv\")\n",
    "\n",
    "# --------------------------\n",
    "# Read CSVs\n",
    "# --------------------------\n",
    "df_resnet = pd.read_csv(resnet_csv)\n",
    "df_yolo_precision = pd.read_csv(yolo_precision_csv)\n",
    "df_yolo_recall = pd.read_csv(yolo_recall_csv)\n",
    "df_yolo_f1 = pd.read_csv(yolo_f1_csv)\n",
    "\n",
    "print(\"1:\",df_yolo_precision.shape)\n",
    "#print(\"1:\",df_yolo_recall)\n",
    "#(\"1:\",df_yolo_f1)\n",
    "# --------------------------\n",
    "# Clean class names (strip whitespace)\n",
    "# --------------------------\n",
    "df_resnet['class'] = df_resnet['class'].str.strip()\n",
    "df_yolo_precision['class'] = df_yolo_precision['class'].str.strip()\n",
    "df_yolo_recall['class'] = df_yolo_recall['class'].str.strip()\n",
    "df_yolo_f1['class'] = df_yolo_f1['class'].str.strip()\n",
    "\n",
    "# --------------------------\n",
    "# Filter ResNet rows to exclude Macro/Micro averages\n",
    "# --------------------------\n",
    "df_resnet_filtered = df_resnet[~df_resnet['class'].isin(['Macro Avg', 'Micro Avg'])].copy()\n",
    "\n",
    "# Identify valid classes (non-zero metrics)\n",
    "valid_classes = df_resnet_filtered[\n",
    "    (df_resnet_filtered['Precision'] > 0) | \n",
    "    (df_resnet_filtered['Recall'] > 0) | \n",
    "    (df_resnet_filtered['F1'] > 0)\n",
    "]['class'].tolist()\n",
    "\n",
    "# Filter ResNet to only valid classes\n",
    "df_resnet = df_resnet_filtered[df_resnet_filtered['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "\n",
    "# Filter YOLO CSVs to match valid classes\n",
    "df_yolo_precision = df_yolo_precision[df_yolo_precision['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "df_yolo_recall = df_yolo_recall[df_yolo_recall['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "df_yolo_f1 = df_yolo_f1[df_yolo_f1['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "\n",
    "# Merge into a single YOLO DataFrame\n",
    "df_yolo = pd.DataFrame({\"class\": valid_classes})\n",
    "df_yolo = df_yolo.merge(df_yolo_precision[['class', 'Precision']], on='class', how='left')\n",
    "df_yolo = df_yolo.merge(df_yolo_recall[['class', 'Recall']], on='class', how='left')\n",
    "df_yolo = df_yolo.merge(df_yolo_f1[['class', 'F1']], on='class', how='left')\n",
    "df_yolo.fillna(0, inplace=True)\n",
    "\n",
    "print(df_resnet)\n",
    "print(df_yolo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e171e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Function to create side-by-side bar charts\n",
    "# --------------------------\n",
    "def plot_metric_comparison(df_resnet, df_yolo, metric, save_path=None):\n",
    "    x = np.arange(len(df_resnet))  # positions for classes\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.bar(x - width/2, df_resnet[metric], width, label='ResNet', color='skyblue')\n",
    "    ax.bar(x + width/2, df_yolo[metric], width, label='YOLO', color='orange')\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_resnet['class'], rotation=45, ha='right')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'Per-Class {metric} Comparison: YOLO vs ResNet')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# Generate bar charts for Precision, Recall, F1\n",
    "# --------------------------\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"Precision\", os.path.join(repo_root, \"comparison_precision.png\"))\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"Recall\", os.path.join(repo_root, \"comparison_recall.png\"))\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"F1\", os.path.join(repo_root, \"comparison_f1.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "851ac541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Grouped bar chart with 6 categories\n",
    "# --------------------------\n",
    "metrics = ['Precision', 'Recall', 'F1']\n",
    "x = np.arange(len(df_resnet))\n",
    "width = 0.12\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,7))\n",
    "\n",
    "# Color-blind friendly palette (Okabe-Ito)\n",
    "metric_colors = {\n",
    "    'Precision': '#0072B2',  # blue\n",
    "    'Recall': '#009E73',     # green\n",
    "    'F1': '#E69F00'          # orange\n",
    "}\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # ResNet\n",
    "    bars_resnet = ax.bar(x + (i-1)*2*width, df_resnet[metric], width,\n",
    "                         color=metric_colors[metric], alpha=0.9)\n",
    "    handles.append(bars_resnet)\n",
    "    labels.append(f\"ResNet {metric}\")\n",
    "\n",
    "    # YOLO\n",
    "    bars_yolo = ax.bar(x + (i-1)*2*width + width, df_yolo[metric], width,\n",
    "                       color=metric_colors[metric], alpha=0.6, hatch='//', edgecolor='black')\n",
    "    handles.append(bars_yolo)\n",
    "    labels.append(f\"YOLO {metric}\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_resnet['class'], rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel(\"Metric Value\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_title(\"Per-Class Precision, Recall, F1: YOLO vs ResNet\", fontsize=14, pad=15)\n",
    "\n",
    "# Full legend\n",
    "ax.legend(handles, labels, fontsize=10, loc='upper right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"comparison_grouped_metrics_clean.png\"), bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35a422b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\precision_recall_scatter.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Scatter plot: Precision vs Recall\n",
    "# --------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# ResNet points (blue circles)\n",
    "plt.scatter(df_resnet['Precision'], df_resnet['Recall'], \n",
    "            color=\"#CC3311\", marker=\"o\", s=100, label=\"ResNet\")\n",
    "\n",
    "# YOLO points (orange triangles)\n",
    "plt.scatter(df_yolo_precision['Precision'], df_yolo_recall['Recall'], \n",
    "            color=\"#666666\", marker=\"^\", s=100, label=\"YOLO\")\n",
    "\n",
    "# Add class labels with larger font\n",
    "for i, cls in enumerate(df_resnet['class']):\n",
    "    plt.text(df_resnet['Precision'][i] + 0.015, df_resnet['Recall'][i] + 0.015, cls,\n",
    "             fontsize=10, color=\"#CC3311\")\n",
    "    plt.text(df_yolo_precision['Precision'][i] + 0.015, df_yolo_recall['Recall'][i] - 0.025, cls,\n",
    "             fontsize=10, color=\"#666666\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(\"Precision\", fontsize=12)\n",
    "plt.ylabel(\"Recall\", fontsize=12)\n",
    "plt.title(\"Precision vs Recall per Class: YOLO vs ResNet\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Set axis limits with small margins so points don't get cut off\n",
    "plt.xlim(0.08, 1.02)\n",
    "plt.ylim(0.08, 1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "save_path = os.path.join(repo_root, \"precision_recall_scatter.png\")\n",
    "plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Scatter plot saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pascalvoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
