{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f95699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Data handling / visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import yaml\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc138c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (11): Concat()\n",
      "      (12): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (14): Concat()\n",
      "      (15): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (17): Concat()\n",
      "      (18): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (20): Concat()\n",
      "      (21): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Setup device\n",
    "# --------------------------\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Load pre-trained YOLOv8 model\n",
    "# --------------------------\n",
    "# Nano YOLOv8 for CPU\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "print(\"Model loaded:\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db607e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO dataset prepared and voc.yaml created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Dataset setup\n",
    "# --------------------------\n",
    "# Paths\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\")\n",
    "yolo_dataset_dir = os.path.join(repo_root, \"YOLO_VOC\")\n",
    "os.makedirs(yolo_dataset_dir, exist_ok=True)\n",
    "\n",
    "# VOC Classes\n",
    "VOC_CLASSES = [\n",
    "    'aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow',\n",
    "    'diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "\n",
    "# Function to convert bounding boxes\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1.0 / size[0]\n",
    "    dh = 1.0 / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    return x*dw, y*dh, w*dw, h*dh\n",
    "\n",
    "# Conversion function\n",
    "def voc_to_yolo(voc_root, year=\"2012\", split=\"train\", output_dir=\"YOLO_VOC\"):\n",
    "    img_dir = os.path.join(voc_root, f\"VOC{year}\", \"JPEGImages\")\n",
    "    ann_dir = os.path.join(voc_root, f\"VOC{year}\", \"Annotations\")\n",
    "    split_file = os.path.join(voc_root, f\"VOC{year}\", \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "\n",
    "    # Output dirs\n",
    "    txt_output_dir = os.path.join(output_dir, split, \"labels\")\n",
    "    img_output_dir = os.path.join(output_dir, split, \"images\")\n",
    "    os.makedirs(txt_output_dir, exist_ok=True)\n",
    "    os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "    # Read official split list\n",
    "    with open(split_file, \"r\") as f:\n",
    "        img_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for img_id in img_ids:\n",
    "        # Copy image\n",
    "        src_img = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
    "        dst_img = os.path.join(img_output_dir, f\"{img_id}.jpg\")\n",
    "        shutil.copy(src_img, dst_img)\n",
    "\n",
    "        # Convert annotation\n",
    "        xml_file = os.path.join(ann_dir, f\"{img_id}.xml\")\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        w = int(root.find(\"size/width\").text)\n",
    "        h = int(root.find(\"size/height\").text)\n",
    "\n",
    "        yolo_labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            cls_name = obj.find(\"name\").text\n",
    "            if cls_name not in VOC_CLASSES:\n",
    "                continue\n",
    "            cls_id = VOC_CLASSES.index(cls_name)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            x_center, y_center, bw, bh = convert_bbox((w, h), (xmin, xmax, ymin, ymax))\n",
    "            yolo_labels.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\")\n",
    "\n",
    "        txt_file_path = os.path.join(txt_output_dir, f\"{img_id}.txt\")\n",
    "        with open(txt_file_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_labels))\n",
    "\n",
    "\n",
    "# Convert train and val sets\n",
    "voc_to_yolo(voc_root, \"2012\", \"train\", yolo_dataset_dir)\n",
    "voc_to_yolo(voc_root, \"2012\", \"val\", yolo_dataset_dir)\n",
    "\n",
    "# Generate YAML config for YOLOv8\n",
    "voc_yaml = {\n",
    "    'train': os.path.join(yolo_dataset_dir, 'train', 'images'),\n",
    "    'val': os.path.join(yolo_dataset_dir, 'val', 'images'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(repo_root, \"voc.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(voc_yaml, f)\n",
    "print(\"YOLO dataset prepared and voc.yaml created at:\", yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4bddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train subset created with 800 images.\n",
      "val subset created with 200 images.\n",
      "Subset YAML created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Subset setup\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\", \"VOC2012\")\n",
    "subset_dir = os.path.join(repo_root, \"YOLO_VOC_subset\")\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "subset_sizes = {\n",
    "    \"train\": 800,   # number of images for CPU testing\n",
    "    \"val\": 200\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Function to create subset\n",
    "# --------------------------\n",
    "def create_yolo_subset(voc_root, original_dir, subset_dir, split, num_images):\n",
    "    split_file = os.path.join(voc_root, \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "    orig_images_dir = os.path.join(original_dir, split, \"images\")\n",
    "    orig_labels_dir = os.path.join(original_dir, split, \"labels\")\n",
    "\n",
    "    subset_images_dir = os.path.join(subset_dir, split, \"images\")\n",
    "    subset_labels_dir = os.path.join(subset_dir, split, \"labels\")\n",
    "    os.makedirs(subset_images_dir, exist_ok=True)\n",
    "    os.makedirs(subset_labels_dir, exist_ok=True)\n",
    "\n",
    "    # Load official split IDs\n",
    "    with open(split_file, \"r\") as f:\n",
    "        split_ids = [line.strip() + \".jpg\" for line in f.readlines()]\n",
    "\n",
    "    # Keep only those that exist in original_dir\n",
    "    available_images = [f for f in split_ids if f in os.listdir(orig_images_dir)]\n",
    "\n",
    "    # Random sample\n",
    "    sampled_images = random.sample(available_images, min(num_images, len(available_images)))\n",
    "    \n",
    "    # Take the first N images instead of random sample\n",
    "    #sampled_images = available_images[:num_images]\n",
    "\n",
    "    for img_file in sampled_images:\n",
    "        # Copy image\n",
    "        shutil.copy(os.path.join(orig_images_dir, img_file),\n",
    "                    os.path.join(subset_images_dir, img_file))\n",
    "        # Copy corresponding label\n",
    "        label_file = img_file.replace(\".jpg\", \".txt\")\n",
    "        shutil.copy(os.path.join(orig_labels_dir, label_file),\n",
    "                    os.path.join(subset_labels_dir, label_file))\n",
    "\n",
    "    print(f\"{split} subset created with {len(sampled_images)} images.\")\n",
    "\n",
    "# --------------------------\n",
    "# Create subsets\n",
    "# --------------------------\n",
    "for split in [\"train\", \"val\"]:\n",
    "    create_yolo_subset(voc_root, yolo_dataset_dir, subset_dir, split, subset_sizes[split])\n",
    "\n",
    "# --------------------------\n",
    "# Create subset YAML\n",
    "# --------------------------\n",
    "subset_yaml = {\n",
    "    'train': os.path.join(subset_dir, 'train'),\n",
    "    'val': os.path.join(subset_dir, 'val'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "subset_yaml_path = os.path.join(repo_root, \"voc_subset.yaml\")\n",
    "with open(subset_yaml_path, \"w\") as f:\n",
    "    yaml.dump(subset_yaml, f)\n",
    "print(\"Subset YAML created at:\", subset_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf68d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.197 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train12, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 148.961.4 MB/s, size: 114.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels... 4116 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 4116/4116 1741.0it/s 2.4s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 195.154.5 MB/s, size: 119.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels... 1227 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1227/1227 1809.9it/s 0.7s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\train12\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train12\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G      1.172      3.109      1.142         15        224: 100% ━━━━━━━━━━━━ 515/515 2.7it/s 3:13<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 3.1it/s 25.2s0.3ss\n",
      "                   all       1227       3381      0.668      0.402      0.451      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10         0G      1.239       1.88      1.177          9        224: 100% ━━━━━━━━━━━━ 515/515 2.7it/s 3:09<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 2.8it/s 27.5s0.4ss\n",
      "                   all       1227       3381      0.604      0.476      0.514      0.351\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10         0G      1.207      1.678      1.179          7        224: 100% ━━━━━━━━━━━━ 515/515 2.7it/s 3:13<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 2.9it/s 26.8s0.4ss\n",
      "                   all       1227       3381      0.624        0.5      0.529      0.362\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10         0G      1.182      1.564      1.171         14        224: 100% ━━━━━━━━━━━━ 515/515 2.8it/s 3:02<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 3.2it/s 24.3s0.3ss\n",
      "                   all       1227       3381       0.64       0.52      0.556       0.38\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10         0G      1.161      1.494      1.161          8        224: 100% ━━━━━━━━━━━━ 515/515 2.7it/s 3:10<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 3.0it/s 25.3s0.3ss\n",
      "                   all       1227       3381      0.652      0.523      0.564      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10         0G      1.127      1.423      1.145          7        224: 100% ━━━━━━━━━━━━ 515/515 2.9it/s 2:58<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 3.1it/s 25.1s0.3ss\n",
      "                   all       1227       3381      0.641      0.531      0.574      0.397\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10         0G      1.098      1.382      1.134         16        224: 100% ━━━━━━━━━━━━ 515/515 2.7it/s 3:12<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 3.0it/s 25.8s0.3ss\n",
      "                   all       1227       3381      0.679       0.53      0.573        0.4\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10         0G      1.074       1.33      1.126         11        224: 100% ━━━━━━━━━━━━ 515/515 2.7it/s 3:12<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 2.3it/s 33.4s0.4ss\n",
      "                   all       1227       3381      0.716      0.519      0.586      0.406\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10         0G      1.059      1.293       1.11          6        224: 100% ━━━━━━━━━━━━ 515/515 2.3it/s 3:41<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 2.6it/s 29.3s0.4ss\n",
      "                   all       1227       3381      0.679      0.523      0.589      0.412\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10         0G      1.043      1.274      1.106         19        224: 100% ━━━━━━━━━━━━ 515/515 2.6it/s 3:19<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 2.5it/s 31.0s0.4ss\n",
      "                   all       1227       3381      0.718      0.525      0.594       0.42\n",
      "\n",
      "10 epochs completed in 0.613 hours.\n",
      "Optimizer stripped from runs\\detect\\train12\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train12\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train12\\weights\\best.pt...\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 77/77 3.0it/s 25.6s0.3ss\n",
      "                   all       1227       3381      0.718      0.524      0.594      0.419\n",
      "             aeroplane         54         93      0.769      0.516      0.558      0.419\n",
      "               bicycle         56         79      0.671      0.515      0.592      0.406\n",
      "                  bird         74        144      0.846       0.38      0.508      0.325\n",
      "                  boat         47         81      0.472      0.464      0.407      0.238\n",
      "                bottle         76        155      0.682      0.342      0.388      0.255\n",
      "                   bus         46         65      0.852      0.677      0.755      0.623\n",
      "                   car        113        252      0.865      0.382       0.55      0.356\n",
      "                   cat        103        118      0.737      0.695      0.792      0.592\n",
      "                 chair        125        308      0.588      0.312      0.406       0.24\n",
      "                   cow         28         60      0.476      0.533      0.538      0.395\n",
      "           diningtable         57         60      0.775        0.7      0.756      0.516\n",
      "                   dog        136        159       0.76      0.572      0.702       0.52\n",
      "                 horse         45         80      0.692      0.475       0.56       0.41\n",
      "             motorbike         52         72      0.717      0.694      0.718      0.541\n",
      "                person        541       1223      0.865      0.549      0.683      0.455\n",
      "           pottedplant         56        125      0.574      0.256       0.29      0.173\n",
      "                 sheep         31         93      0.713      0.495      0.577       0.36\n",
      "                  sofa         75         89      0.713      0.506      0.605      0.441\n",
      "                 train         53         56      0.897      0.804      0.866      0.683\n",
      "             tvmonitor         55         69      0.695      0.609      0.625      0.439\n",
      "Speed: 0.2ms preprocess, 14.4ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Fine-tune model (CPU-friendly)\n",
    "# --------------------------\n",
    "results = model.train(\n",
    "    data=subset_yaml_path,      #yaml_path for full dataset; subset_yaml_path for debugging/CPU\n",
    "    epochs=10,            # lower for CPU\n",
    "    batch=8,             # small batch for CPU\n",
    "    imgsz=224,           # smaller image size speeds up CPU training\n",
    "    device=device,        # CPU\n",
    "    freeze=10           #freezes first 10 layers (backbone)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6981a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 306.9156.1 MB/s, size: 145.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache... 1227 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1227/1227  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 154/154 4.9it/s 31.3s0.2ss\n",
      "                   all       1227       3381      0.617      0.611      0.619      0.449\n",
      "             aeroplane         54         93      0.658      0.559      0.592      0.468\n",
      "               bicycle         56         79      0.581      0.633      0.607      0.456\n",
      "                  bird         74        144      0.574      0.479      0.525      0.341\n",
      "                  boat         47         81      0.556      0.531      0.497      0.316\n",
      "                bottle         76        155      0.615      0.432      0.486      0.325\n",
      "                   bus         46         65      0.678      0.738      0.777      0.605\n",
      "                   car        113        252      0.774      0.512      0.672       0.47\n",
      "                   cat        103        118      0.731       0.78      0.809      0.633\n",
      "                 chair        125        308       0.53      0.451      0.426      0.264\n",
      "                   cow         28         60      0.439      0.667      0.561      0.432\n",
      "           diningtable         57         60      0.501      0.783      0.666      0.433\n",
      "                   dog        136        159      0.686      0.648      0.746      0.586\n",
      "                 horse         45         80      0.634      0.575       0.58      0.452\n",
      "             motorbike         52         72      0.634      0.778      0.721       0.55\n",
      "                person        541       1223      0.816      0.612      0.739      0.522\n",
      "           pottedplant         56        125      0.492       0.32      0.338      0.189\n",
      "                 sheep         31         93      0.663      0.614      0.649      0.466\n",
      "                  sofa         75         89      0.566      0.543      0.579      0.395\n",
      "                 train         53         56      0.579      0.821      0.794      0.648\n",
      "             tvmonitor         55         69      0.623      0.739      0.621       0.43\n",
      "Speed: 0.2ms preprocess, 20.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train123\u001b[0m\n",
      "Overall mAP@0.5:0.95: 0.4491727131865913\n",
      "Overall Precision: 0.6165006060400222\n",
      "Overall Recall: 0.610764940095976\n",
      "\n",
      "Per-class mAP:\n",
      "                class  mAP@0.5:0.95\n",
      "0          aeroplane      0.468104\n",
      "1            bicycle      0.456455\n",
      "2               bird      0.340544\n",
      "3               boat      0.315759\n",
      "4             bottle      0.324681\n",
      "5                bus      0.605270\n",
      "6                car      0.470282\n",
      "7                cat      0.633147\n",
      "8              chair      0.264456\n",
      "9                cow      0.431516\n",
      "10       diningtable      0.433145\n",
      "11               dog      0.586416\n",
      "12             horse      0.452174\n",
      "13         motorbike      0.550337\n",
      "14            person      0.522270\n",
      "15       pottedplant      0.189420\n",
      "16             sheep      0.466445\n",
      "17              sofa      0.395115\n",
      "18             train      0.648362\n",
      "19         tvmonitor      0.429556\n",
      "Overall      Overall      0.449173\n",
      "\n",
      "Per-class Precision:\n",
      "                class  Precision\n",
      "0          aeroplane   0.658234\n",
      "1            bicycle   0.580772\n",
      "2               bird   0.573947\n",
      "3               boat   0.556402\n",
      "4             bottle   0.614758\n",
      "5                bus   0.677875\n",
      "6                car   0.773908\n",
      "7                cat   0.730867\n",
      "8              chair   0.530063\n",
      "9                cow   0.439454\n",
      "10       diningtable   0.501343\n",
      "11               dog   0.686224\n",
      "12             horse   0.633580\n",
      "13         motorbike   0.633622\n",
      "14            person   0.816038\n",
      "15       pottedplant   0.492068\n",
      "16             sheep   0.663086\n",
      "17              sofa   0.566435\n",
      "18             train   0.578799\n",
      "19         tvmonitor   0.622537\n",
      "Overall      Overall   0.616501\n",
      "\n",
      "Per-class Recall:\n",
      "                class    Recall\n",
      "0          aeroplane  0.559140\n",
      "1            bicycle  0.632911\n",
      "2               bird  0.479167\n",
      "3               boat  0.530864\n",
      "4             bottle  0.432258\n",
      "5                bus  0.738462\n",
      "6                car  0.511905\n",
      "7                cat  0.779661\n",
      "8              chair  0.451299\n",
      "9                cow  0.666667\n",
      "10       diningtable  0.783333\n",
      "11               dog  0.647799\n",
      "12             horse  0.575000\n",
      "13         motorbike  0.777778\n",
      "14            person  0.611611\n",
      "15       pottedplant  0.320000\n",
      "16             sheep  0.613722\n",
      "17              sofa  0.543164\n",
      "18             train  0.821429\n",
      "19         tvmonitor  0.739130\n",
      "Overall      Overall  0.610765\n",
      "\n",
      "Per-class F1:\n",
      "           class        F1\n",
      "0     aeroplane  0.604653\n",
      "1       bicycle  0.605721\n",
      "2          bird  0.522291\n",
      "3          boat  0.543333\n",
      "4        bottle  0.507602\n",
      "5           bus  0.706872\n",
      "6           car  0.616213\n",
      "7           cat  0.754476\n",
      "8         chair  0.487520\n",
      "9           cow  0.529724\n",
      "10  diningtable  0.611389\n",
      "11          dog  0.666457\n",
      "12        horse  0.602870\n",
      "13    motorbike  0.698337\n",
      "14       person  0.699188\n",
      "15  pottedplant  0.387804\n",
      "16        sheep  0.637449\n",
      "17         sofa  0.554555\n",
      "18        train  0.679092\n",
      "19    tvmonitor  0.675841\n",
      "20      Overall  0.613619\n",
      "All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Compute full validation metrics\n",
    "# --------------------------\n",
    "metrics = model.val(\n",
    "    data=subset_yaml_path,\n",
    "    split=\"val\",\n",
    "    imgsz=320,\n",
    "    conf=0.1\n",
    ")\n",
    "\n",
    "print(\"Overall mAP@0.5:0.95:\", metrics.box.map)\n",
    "print(\"Overall Precision:\", metrics.box.mp)\n",
    "print(\"Overall Recall:\", metrics.box.mr)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 6. Export per-class metrics into DataFrames\n",
    "# --------------------------\n",
    "\n",
    "# mAP DataFrame\n",
    "df_map = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"mAP@0.5:0.95\": metrics.box.maps\n",
    "})\n",
    "df_map.loc[\"Overall\"] = [\"Overall\", metrics.box.map]\n",
    "\n",
    "# Precision DataFrame\n",
    "df_precision = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Precision\": metrics.box.p\n",
    "})\n",
    "df_precision.loc[\"Overall\"] = [\"Overall\", metrics.box.mp]\n",
    "\n",
    "# Recall DataFrame\n",
    "df_recall = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Recall\": metrics.box.r\n",
    "})\n",
    "df_recall.loc[\"Overall\"] = [\"Overall\", metrics.box.mr]\n",
    "\n",
    "# F1 DataFrame\n",
    "f1_per_class = 2 * (metrics.box.p * metrics.box.r) / (metrics.box.p + metrics.box.r + 1e-6)\n",
    "overall_f1 = 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr + 1e-6)\n",
    "df_f1 = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"F1\": f1_per_class\n",
    "})\n",
    "df_f1.loc[len(df_f1)] = [\"Overall\", overall_f1]\n",
    "\n",
    "print(\"\\nPer-class mAP:\\n\", df_map)\n",
    "print(\"\\nPer-class Precision:\\n\", df_precision)\n",
    "print(\"\\nPer-class Recall:\\n\", df_recall)\n",
    "print(\"\\nPer-class F1:\\n\", df_f1)\n",
    "\n",
    "# Save CSVs in results/YOLO_object_detection_results\n",
    "object_detection_results_dir = os.path.join(repo_root, \"results\", \"YOLO_object_detection_results\")\n",
    "os.makedirs(object_detection_results_dir, exist_ok=True)\n",
    "\n",
    "# Save to CSVs\n",
    "df_map.to_csv(os.path.join(object_detection_results_dir, \"yolo_val_map.csv\"), index=False)\n",
    "df_precision.to_csv(os.path.join(object_detection_results_dir, \"yolo_val_precision.csv\"), index=False)\n",
    "df_recall.to_csv(os.path.join(object_detection_results_dir, \"yolo_val_recall.csv\"), index=False)\n",
    "df_f1.to_csv(os.path.join(object_detection_results_dir, \"yolo_val_f1.csv\"), index=False)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 7. Combined grouped bar chart (Precision, Recall, F1)\n",
    "# --------------------------\n",
    "classes = df_precision[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))  # positions\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width, df_precision[\"Precision\"][:-1], width=width, label=\"Precision\")\n",
    "plt.bar(x, df_recall[\"Recall\"][:-1], width=width, label=\"Recall\")\n",
    "plt.bar(x + width, df_f1[\"F1\"][:-1], width=width, label=\"F1\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Precision, Recall, and F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(object_detection_results_dir, \"yolo_perclass_prf1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7f5359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr 0.9229321054127909\n",
      "pval 6.869658006878054e-09\n",
      "Generated scatterplot: Per-class mAP vs F1 (with correlation)\n"
     ]
    }
   ],
   "source": [
    "# Exclude \"Overall\" row\n",
    "perclass_map = df_map[\"mAP@0.5:0.95\"][:-1]\n",
    "perclass_f1 = df_f1[\"F1\"][:-1]\n",
    "classes = df_map[\"class\"][:-1]\n",
    "\n",
    "# Compute correlation\n",
    "corr, pval = pearsonr(perclass_map, perclass_f1)\n",
    "print(\"corr\", corr)\n",
    "print(\"pval\", pval)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Scatter with bigger dots and specific color\n",
    "sns.scatterplot(\n",
    "    x=perclass_map,\n",
    "    y=perclass_f1,\n",
    "    s=120,                # bigger dots\n",
    "    color=\"#009E73\"       # custom color\n",
    ")\n",
    "\n",
    "# Annotate each class with larger, colored labels\n",
    "for i, cls in enumerate(classes):\n",
    "    plt.text(\n",
    "        perclass_map.iloc[i] + 0.005,\n",
    "        perclass_f1.iloc[i] + 0.005,\n",
    "        cls,\n",
    "        fontsize=8,        # larger font\n",
    "        color=\"#009E73\"\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"mAP@0.5:0.95\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(f\"YOLOv8 Per-Class mAP vs F1 (Validation)\\nPearson r={corr:.2f}, p={pval:.3f}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "\n",
    "plt.savefig(os.path.join(object_detection_results_dir, \"yolo_map_vs_f1_scatter.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated scatterplot: Per-class mAP vs F1 (with correlation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f972399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated grouped bar chart: Per-class mAP vs F1\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Compare per-class mAP vs F1\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "classes = df_map[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35  # bar width\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width/2, df_map[\"mAP@0.5:0.95\"][:-1], width=width, label=\"mAP@0.5:0.95\")\n",
    "plt.bar(x + width/2, df_f1[\"F1\"][:-1], width=width, label=\"F1 Score\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Comparison: mAP vs F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(object_detection_results_dir, \"yolo_perclass_map_vs_f1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated grouped bar chart: Per-class mAP vs F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4910bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_01.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_02.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_03.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_04.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_05.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_06.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_07.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_08.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_09.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\results\\YOLO_object_detection_results\\comparison_predictions\\comparison_10.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Side-by-side GT vs Predictions (10 random val images)\n",
    "# --------------------------\n",
    "\n",
    "val_images_dir = os.path.join(subset_dir, \"val\", \"images\")\n",
    "val_labels_dir = os.path.join(subset_dir, \"val\", \"labels\")\n",
    "\n",
    "# Randomly select 10 val images\n",
    "all_val_images = glob.glob(os.path.join(val_images_dir, \"*.jpg\"))\n",
    "sample_val_images = random.sample(all_val_images, min(10, len(all_val_images)))\n",
    "\n",
    "comparison_dir = os.path.join(object_detection_results_dir, \"comparison_predictions\")\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "for idx, img_path in enumerate(sample_val_images, start=1):\n",
    "    # Load GT labels\n",
    "    label_path = os.path.join(val_labels_dir, os.path.basename(img_path).replace(\".jpg\", \".txt\"))\n",
    "    gt_boxes = []\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                cls_id, x, y, w, h = line.strip().split()\n",
    "                cls_id = int(cls_id)\n",
    "                gt_boxes.append((VOC_CLASSES[cls_id], float(x), float(y), float(w), float(h)))\n",
    "\n",
    "    # Run YOLO prediction\n",
    "    results = model.predict(source=img_path, imgsz=320, conf=0.1, verbose=False)\n",
    "    res = results[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Left: Ground truth (draw bounding boxes manually)\n",
    "    img = mpimg.imread(img_path)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    h, w = img.shape[:2]\n",
    "    for cls_name, x, y, bw, bh in gt_boxes:\n",
    "        xmin = int((x - bw/2) * w)\n",
    "        ymin = int((y - bh/2) * h)\n",
    "        xmax = int((x + bw/2) * w)\n",
    "        ymax = int((y + bh/2) * h)\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin,\n",
    "                             linewidth=2, edgecolor=\"lime\", facecolor=\"none\")\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].text(xmin, ymin-5, cls_name, color=\"lime\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "    # Right: YOLO predictions (ultralytics res.plot())\n",
    "    img_pred = res.plot()\n",
    "    axes[1].imshow(img_pred)\n",
    "    axes[1].set_title(\"YOLO Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_dir, f\"comparison_{idx:02d}.png\")\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved GT vs Prediction comparison: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: (21, 2)\n",
      "       class  Precision    Recall        F1\n",
      "0  aeroplane   0.500000  0.250000  0.333333\n",
      "1    bicycle   1.000000  0.250000  0.400000\n",
      "2       bird   0.800000  0.500000  0.615385\n",
      "3        car   0.600000  0.315789  0.413793\n",
      "4        cat   0.916667  0.687500  0.785714\n",
      "5      chair   0.538462  0.411765  0.466667\n",
      "6      horse   1.000000  0.250000  0.400000\n",
      "7  motorbike   1.000000  0.125000  0.222222\n",
      "8     person   0.835821  0.910569  0.871595\n",
      "9  tvmonitor   0.500000  0.571429  0.533333\n",
      "       class  Precision    Recall        F1\n",
      "0  aeroplane   0.000000  0.000000  0.000000\n",
      "1    bicycle   0.000000  0.000000  0.000000\n",
      "2       bird   0.000000  0.000000  0.000000\n",
      "3        car   0.062731  0.093407  0.075055\n",
      "4        cat   0.050725  0.093333  0.065727\n",
      "5      chair   0.000000  0.000000  0.000000\n",
      "6      horse   0.000000  0.000000  0.000000\n",
      "7  motorbike   0.000000  0.000000  0.000000\n",
      "8     person   0.186667  0.245077  0.211920\n",
      "9  tvmonitor   0.000000  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Comparing YOLO and ResNet's classification performance on Pascal VOC\n",
    "# --------------------------\n",
    "# \n",
    "# # --------------------------\n",
    "# Paths to CSV files\n",
    "# --------------------------\n",
    "\n",
    "resnet_csv = os.path.join(object_detection_results_dir, \"resnet_val_precision_recall_f1.csv\")\n",
    "yolo_precision_csv = os.path.join(object_detection_results_dir, \"yolo_val_precision.csv\")\n",
    "yolo_recall_csv = os.path.join(object_detection_results_dir, \"yolo_val_recall.csv\")\n",
    "yolo_f1_csv = os.path.join(object_detection_results_dir, \"yolo_val_f1.csv\")\n",
    "\n",
    "# --------------------------\n",
    "# Read CSVs\n",
    "# --------------------------\n",
    "df_resnet = pd.read_csv(resnet_csv)\n",
    "df_yolo_precision = pd.read_csv(yolo_precision_csv)\n",
    "df_yolo_recall = pd.read_csv(yolo_recall_csv)\n",
    "df_yolo_f1 = pd.read_csv(yolo_f1_csv)\n",
    "\n",
    "print(\"1:\",df_yolo_precision.shape)\n",
    "#print(\"1:\",df_yolo_recall)\n",
    "#(\"1:\",df_yolo_f1)\n",
    "# --------------------------\n",
    "# Clean class names (strip whitespace)\n",
    "# --------------------------\n",
    "df_resnet['class'] = df_resnet['class'].str.strip()\n",
    "df_yolo_precision['class'] = df_yolo_precision['class'].str.strip()\n",
    "df_yolo_recall['class'] = df_yolo_recall['class'].str.strip()\n",
    "df_yolo_f1['class'] = df_yolo_f1['class'].str.strip()\n",
    "\n",
    "# --------------------------\n",
    "# Filter ResNet rows to exclude Macro/Micro averages\n",
    "# --------------------------\n",
    "df_resnet_filtered = df_resnet[~df_resnet['class'].isin(['Macro Avg', 'Micro Avg'])].copy()\n",
    "\n",
    "# Identify valid classes (non-zero metrics)\n",
    "valid_classes = df_resnet_filtered[\n",
    "    (df_resnet_filtered['Precision'] > 0) | \n",
    "    (df_resnet_filtered['Recall'] > 0) | \n",
    "    (df_resnet_filtered['F1'] > 0)\n",
    "]['class'].tolist()\n",
    "\n",
    "# Filter ResNet to only valid classes\n",
    "df_resnet = df_resnet_filtered[df_resnet_filtered['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "\n",
    "# Filter YOLO CSVs to match valid classes\n",
    "df_yolo_precision = df_yolo_precision[df_yolo_precision['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "df_yolo_recall = df_yolo_recall[df_yolo_recall['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "df_yolo_f1 = df_yolo_f1[df_yolo_f1['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "\n",
    "# Merge into a single YOLO DataFrame\n",
    "df_yolo = pd.DataFrame({\"class\": valid_classes})\n",
    "df_yolo = df_yolo.merge(df_yolo_precision[['class', 'Precision']], on='class', how='left')\n",
    "df_yolo = df_yolo.merge(df_yolo_recall[['class', 'Recall']], on='class', how='left')\n",
    "df_yolo = df_yolo.merge(df_yolo_f1[['class', 'F1']], on='class', how='left')\n",
    "df_yolo.fillna(0, inplace=True)\n",
    "\n",
    "print(df_resnet)\n",
    "print(df_yolo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e171e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Function to create side-by-side bar charts\n",
    "# --------------------------\n",
    "def plot_metric_comparison(df_resnet, df_yolo, metric, save_path=None):\n",
    "    x = np.arange(len(df_resnet))  # positions for classes\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.bar(x - width/2, df_resnet[metric], width, label='ResNet', color='skyblue')\n",
    "    ax.bar(x + width/2, df_yolo[metric], width, label='YOLO', color='orange')\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_resnet['class'], rotation=45, ha='right')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'Per-Class {metric} Comparison: YOLO vs ResNet')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# Generate bar charts for Precision, Recall, F1\n",
    "# --------------------------\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"Precision\", os.path.join(object_detection_results_dir, \"comparison_precision.png\"))\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"Recall\", os.path.join(object_detection_results_dir, \"comparison_recall.png\"))\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"F1\", os.path.join(object_detection_results_dir, \"comparison_f1.png\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ac541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Grouped bar chart with 6 categories\n",
    "# --------------------------\n",
    "metrics = ['Precision', 'Recall', 'F1']\n",
    "x = np.arange(len(df_resnet))\n",
    "width = 0.12\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,7))\n",
    "\n",
    "# Color-blind friendly palette (Okabe-Ito)\n",
    "metric_colors = {\n",
    "    'Precision': '#0072B2',  # blue\n",
    "    'Recall': '#009E73',     # green\n",
    "    'F1': '#E69F00'          # orange\n",
    "}\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # ResNet\n",
    "    bars_resnet = ax.bar(x + (i-1)*2*width, df_resnet[metric], width,\n",
    "                         color=metric_colors[metric], alpha=0.9)\n",
    "    handles.append(bars_resnet)\n",
    "    labels.append(f\"ResNet {metric}\")\n",
    "\n",
    "    # YOLO\n",
    "    bars_yolo = ax.bar(x + (i-1)*2*width + width, df_yolo[metric], width,\n",
    "                       color=metric_colors[metric], alpha=0.6, hatch='//', edgecolor='black')\n",
    "    handles.append(bars_yolo)\n",
    "    labels.append(f\"YOLO {metric}\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_resnet['class'], rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel(\"Metric Value\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_title(\"Per-Class Precision, Recall, F1: YOLO vs ResNet\", fontsize=14, pad=15)\n",
    "\n",
    "# Full legend\n",
    "ax.legend(handles, labels, fontsize=10, loc='upper right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(object_detection_results_dir, \"comparison_grouped_metrics_clean.png\"), bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a422b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\precision_recall_scatter.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Scatter plot: Precision vs Recall\n",
    "# --------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# ResNet points (blue circles)\n",
    "plt.scatter(df_resnet['Precision'], df_resnet['Recall'], \n",
    "            color=\"#CC3311\", marker=\"o\", s=100, label=\"ResNet\")\n",
    "\n",
    "# YOLO points (orange triangles)\n",
    "plt.scatter(df_yolo_precision['Precision'], df_yolo_recall['Recall'], \n",
    "            color=\"#666666\", marker=\"^\", s=100, label=\"YOLO\")\n",
    "\n",
    "# Add class labels with larger font\n",
    "for i, cls in enumerate(df_resnet['class']):\n",
    "    plt.text(df_resnet['Precision'][i] + 0.015, df_resnet['Recall'][i] + 0.015, cls,\n",
    "             fontsize=10, color=\"#CC3311\")\n",
    "    plt.text(df_yolo_precision['Precision'][i] + 0.015, df_yolo_recall['Recall'][i] - 0.025, cls,\n",
    "             fontsize=10, color=\"#666666\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(\"Precision\", fontsize=12)\n",
    "plt.ylabel(\"Recall\", fontsize=12)\n",
    "plt.title(\"Precision vs Recall per Class: YOLO vs ResNet\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Set axis limits with small margins so points don't get cut off\n",
    "plt.xlim(0.08, 1.02)\n",
    "plt.ylim(0.08, 1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "save_path = os.path.join(object_detection_results_dir, \"precision_recall_scatter.png\")\n",
    "plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Scatter plot saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pascalvoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
