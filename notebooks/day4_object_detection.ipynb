{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f95699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Data handling / visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import yaml\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc138c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (11): Concat()\n",
      "      (12): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (14): Concat()\n",
      "      (15): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (17): Concat()\n",
      "      (18): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (20): Concat()\n",
      "      (21): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Setup device\n",
    "# --------------------------\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Load pre-trained YOLOv8 model\n",
    "# --------------------------\n",
    "# Nano YOLOv8 for CPU\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "print(\"Model loaded:\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db607e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(yolo_labels))\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Convert train and val sets\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[43mvoc_to_yolo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoc_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2012\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myolo_dataset_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m voc_to_yolo(voc_root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2012\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, yolo_dataset_dir)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Generate YAML config for YOLOv8\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mvoc_to_yolo\u001b[1;34m(voc_root, year, image_set, output_dir)\u001b[0m\n\u001b[0;32m     33\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(img_output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Copy images\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     37\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy(img_file, img_output_dir)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Convert XML annotations\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Dataset setup\n",
    "# --------------------------\n",
    "# Paths\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\")\n",
    "yolo_dataset_dir = os.path.join(repo_root, \"YOLO_VOC\")\n",
    "os.makedirs(yolo_dataset_dir, exist_ok=True)\n",
    "\n",
    "# VOC Classes\n",
    "VOC_CLASSES = [\n",
    "    'aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow',\n",
    "    'diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "\n",
    "# Function to convert bounding boxes\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1.0 / size[0]\n",
    "    dh = 1.0 / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    return x*dw, y*dh, w*dw, h*dh\n",
    "\n",
    "# Conversion function\n",
    "def voc_to_yolo(voc_root, year, image_set, output_dir):\n",
    "    img_dir = os.path.join(voc_root, f\"VOC{year}\", \"JPEGImages\")\n",
    "    ann_dir = os.path.join(voc_root, f\"VOC{year}\", \"Annotations\")\n",
    "    txt_output_dir = os.path.join(output_dir, image_set, \"labels\")\n",
    "    os.makedirs(txt_output_dir, exist_ok=True)\n",
    "    img_output_dir = os.path.join(output_dir, image_set, \"images\")\n",
    "    os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "    # Copy images\n",
    "    for img_file in glob.glob(os.path.join(img_dir, \"*.jpg\")):\n",
    "        shutil.copy(img_file, img_output_dir)\n",
    "\n",
    "    # Convert XML annotations\n",
    "    for xml_file in glob.glob(os.path.join(ann_dir, \"*.xml\")):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        img_name = root.find(\"filename\").text\n",
    "        w = int(root.find(\"size/width\").text)\n",
    "        h = int(root.find(\"size/height\").text)\n",
    "\n",
    "        yolo_labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            cls_name = obj.find(\"name\").text\n",
    "            if cls_name not in VOC_CLASSES:\n",
    "                continue\n",
    "            cls_id = VOC_CLASSES.index(cls_name)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            x_center, y_center, bw, bh = convert_bbox((w,h), (xmin,xmax,ymin,ymax))\n",
    "            yolo_labels.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\")\n",
    "\n",
    "        txt_file_path = os.path.join(txt_output_dir, img_name.replace(\".jpg\",\".txt\"))\n",
    "        with open(txt_file_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_labels))\n",
    "\n",
    "# Convert train and val sets\n",
    "voc_to_yolo(voc_root, \"2012\", \"train\", yolo_dataset_dir)\n",
    "voc_to_yolo(voc_root, \"2012\", \"val\", yolo_dataset_dir)\n",
    "\n",
    "# Generate YAML config for YOLOv8\n",
    "voc_yaml = {\n",
    "    'train': os.path.join(yolo_dataset_dir, 'train', 'images'),\n",
    "    'val': os.path.join(yolo_dataset_dir, 'val', 'images'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(repo_root, \"voc.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(voc_yaml, f)\n",
    "print(\"YOLO dataset prepared and voc.yaml created at:\", yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4bddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train subset created with 500 images.\n",
      "val subset created with 100 images.\n",
      "Subset YAML created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Subset setup\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\")\n",
    "subset_dir = os.path.join(repo_root, \"YOLO_VOC_subset\")\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "subset_sizes = {\n",
    "    \"train\": 500,   # number of images for CPU testing\n",
    "    \"val\": 100\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Function to create subset\n",
    "# --------------------------\n",
    "def create_yolo_subset(original_dir, subset_dir, split, num_images):\n",
    "    orig_images_dir = os.path.join(original_dir, split, \"images\")\n",
    "    orig_labels_dir = os.path.join(original_dir, split, \"labels\")\n",
    "    \n",
    "    subset_images_dir = os.path.join(subset_dir, split, \"images\")\n",
    "    subset_labels_dir = os.path.join(subset_dir, split, \"labels\")\n",
    "    os.makedirs(subset_images_dir, exist_ok=True)\n",
    "    os.makedirs(subset_labels_dir, exist_ok=True)\n",
    "    \n",
    "    all_images = os.listdir(orig_images_dir)\n",
    "    sampled_images = random.sample(all_images, min(num_images, len(all_images)))\n",
    "    \n",
    "    for img_file in sampled_images:\n",
    "        # Copy image\n",
    "        shutil.copy(os.path.join(orig_images_dir, img_file),\n",
    "                    os.path.join(subset_images_dir, img_file))\n",
    "        # Copy corresponding label\n",
    "        label_file = img_file.replace(\".jpg\", \".txt\")\n",
    "        shutil.copy(os.path.join(orig_labels_dir, label_file),\n",
    "                    os.path.join(subset_labels_dir, label_file))\n",
    "    \n",
    "    print(f\"{split} subset created with {len(sampled_images)} images.\")\n",
    "\n",
    "# --------------------------\n",
    "# Create subsets\n",
    "# --------------------------\n",
    "for split in [\"train\", \"val\"]:\n",
    "    create_yolo_subset(yolo_dataset_dir, subset_dir, split, subset_sizes[split])\n",
    "\n",
    "# --------------------------\n",
    "# Create subset YAML\n",
    "# --------------------------\n",
    "subset_yaml = {\n",
    "    'train': os.path.join(subset_dir, 'train'),\n",
    "    'val': os.path.join(subset_dir, 'val'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "subset_yaml_path = os.path.join(repo_root, \"voc_subset.yaml\")\n",
    "with open(subset_yaml_path, \"w\") as f:\n",
    "    yaml.dump(subset_yaml, f)\n",
    "print(\"Subset YAML created at:\", subset_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdf68d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train9, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 288.4235.7 MB/s, size: 91.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels... 1078 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 1078/1078 1384.3it/s 0.8s1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 540.3379.9 MB/s, size: 113.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels... 219 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 219/219 1650.3it/s 0.1s.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\train9\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train9\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G      1.304      3.837      1.239          3        224: 100% ━━━━━━━━━━━━ 270/270 2.4it/s 1:52<0.6ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 2.5it/s 11.1s0.5s\n",
      "                   all        219        540       0.36      0.088     0.0573     0.0394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10         0G      1.336      3.051      1.252          2        224: 100% ━━━━━━━━━━━━ 270/270 1.7it/s 2:36<0.4s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 3.1it/s 8.9s0.4s\n",
      "                   all        219        540      0.493      0.124      0.136     0.0907\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10         0G      1.381      2.735      1.276          6        224: 100% ━━━━━━━━━━━━ 270/270 1.9it/s 2:25<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 3.5it/s 7.9s0.3s\n",
      "                   all        219        540      0.414      0.243      0.208      0.134\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10         0G      1.344      2.507      1.272          4        224: 100% ━━━━━━━━━━━━ 270/270 1.8it/s 2:31<0.6s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 2.5it/s 11.2s0.4s\n",
      "                   all        219        540      0.341      0.241       0.21      0.137\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10         0G      1.349      2.359      1.271          4        224: 100% ━━━━━━━━━━━━ 270/270 1.3it/s 3:23<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 3.6it/s 7.7s0.3s\n",
      "                   all        219        540      0.418      0.238      0.238      0.144\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10         0G      1.306      2.255      1.256          6        224: 100% ━━━━━━━━━━━━ 270/270 1.7it/s 2:38<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 2.4it/s 11.7s0.5s\n",
      "                   all        219        540      0.458      0.281       0.29      0.198\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10         0G      1.258      2.163      1.242          4        224: 100% ━━━━━━━━━━━━ 270/270 1.1it/s 4:077<1.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 1.3it/s 21.3s0.8s\n",
      "                   all        219        540      0.328      0.374      0.286      0.196\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10         0G      1.243      2.053      1.213          2        224: 100% ━━━━━━━━━━━━ 270/270 1.0it/s 4:23<0.6ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 2.2it/s 12.5s0.6s\n",
      "                   all        219        540      0.466      0.323      0.314      0.217\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10         0G      1.228      2.016      1.209          5        224: 100% ━━━━━━━━━━━━ 270/270 1.3it/s 3:34<0.7ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 2.3it/s 12.4s0.6s\n",
      "                   all        219        540      0.545      0.308      0.341      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10         0G      1.217      1.969      1.198          4        224: 100% ━━━━━━━━━━━━ 270/270 1.6it/s 2:44<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 2.9it/s 9.8s0.4s\n",
      "                   all        219        540      0.452      0.366      0.337      0.233\n",
      "\n",
      "10 epochs completed in 0.538 hours.\n",
      "Optimizer stripped from runs\\detect\\train9\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train9\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train9\\weights\\best.pt...\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 28/28 4.1it/s 6.8s0.2s\n",
      "                   all        219        540      0.545      0.309      0.341      0.234\n",
      "             aeroplane          8         29      0.437      0.069      0.101     0.0366\n",
      "               bicycle          6          8      0.356      0.486      0.448      0.258\n",
      "                  bird          8         11      0.556      0.182      0.448      0.326\n",
      "                  boat          7         10      0.399        0.1      0.176       0.14\n",
      "                bottle          9         23          1          0      0.164        0.1\n",
      "                   bus         10         17      0.857      0.529      0.655      0.495\n",
      "                   car         10         19      0.388      0.158       0.21      0.155\n",
      "                   cat         15         20      0.518       0.55      0.466      0.282\n",
      "                 chair         18         41      0.401      0.268      0.217      0.113\n",
      "                   cow          2          2          0          0     0.0622     0.0387\n",
      "           diningtable          7          7      0.174      0.429      0.187     0.0916\n",
      "                   dog         19         27      0.507      0.444       0.46      0.355\n",
      "                 horse          7         21      0.498      0.286      0.296      0.263\n",
      "             motorbike          5          8       0.37      0.375      0.233      0.159\n",
      "                person        122        240      0.788      0.248      0.529      0.336\n",
      "           pottedplant          9         16          1      0.115      0.173      0.106\n",
      "                 sheep          5          8          1       0.21      0.481      0.333\n",
      "                  sofa         10         12      0.447      0.417      0.364      0.268\n",
      "                 train         11         12       0.63       0.75      0.728      0.576\n",
      "             tvmonitor          7          9      0.572      0.556       0.42      0.249\n",
      "Speed: 0.2ms preprocess, 22.4ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Fine-tune model (CPU-friendly)\n",
    "# --------------------------\n",
    "results = model.train(\n",
    "    data=subset_yaml_path,      #yaml_path for full dataset; subset_yaml_path for debugging\n",
    "    epochs=10,            # lower for CPU\n",
    "    batch=4,             # small batch for CPU\n",
    "    imgsz=224,           # smaller image size speeds up CPU training\n",
    "    device=device         # CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90642684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation images for portfolio: ['C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\\\YOLO_VOC_subset\\\\val\\\\images\\\\2007_000123.jpg', 'C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\\\YOLO_VOC_subset\\\\val\\\\images\\\\2007_000768.jpg', 'C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\\\YOLO_VOC_subset\\\\val\\\\images\\\\2007_002427.jpg', 'C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\\\YOLO_VOC_subset\\\\val\\\\images\\\\2007_003207.jpg', 'C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\\\YOLO_VOC_subset\\\\val\\\\images\\\\2007_003286.jpg']\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_000123.jpg: 256x320 1 train, 61.4ms\n",
      "Speed: 1.1ms preprocess, 61.4ms inference, 2.0ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio16\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_000768.jpg: 256x320 2 buss, 1 car, 46.4ms\n",
      "Speed: 1.1ms preprocess, 46.4ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio17\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_002427.jpg: 256x320 1 chair, 1 sofa, 45.5ms\n",
      "Speed: 0.7ms preprocess, 45.5ms inference, 0.7ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio18\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_003207.jpg: 224x320 1 bottle, 4 chairs, 5 diningtables, 1 person, 48.0ms\n",
      "Speed: 0.7ms preprocess, 48.0ms inference, 0.7ms postprocess per image at shape (1, 3, 224, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio19\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_003286.jpg: 256x320 1 bus, 1 train, 62.4ms\n",
      "Speed: 0.8ms preprocess, 62.4ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio20\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Directories and run predictions \n",
    "# --------------------------\n",
    "output_dir = os.path.join(repo_root, \"yolo_predictions_demo\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "val_images_dir = os.path.join(subset_dir, \"val\", \"images\")\n",
    "val_images = glob.glob(os.path.join(val_images_dir, \"*.jpg\"))[:5]\n",
    "print(\"Validation images for portfolio:\", val_images)\n",
    "\n",
    "predictions = []\n",
    "results_list = []\n",
    "for img_path in val_images:\n",
    "    results = model.predict(\n",
    "        source=img_path,\n",
    "        imgsz=320,\n",
    "        conf=0.1,\n",
    "        save=True,\n",
    "        project=output_dir,\n",
    "        name=\"demo_portfolio\"\n",
    "    )\n",
    "    # Determine predicted image path\n",
    "    pred_folder_base = os.path.join(output_dir, \"demo_portfolio\", \"predict\")\n",
    "    pred_folders = sorted([d for d in glob.glob(pred_folder_base + \"*\") if os.path.isdir(d)])\n",
    "    if pred_folders:\n",
    "        pred_folder = os.path.join(pred_folders[-1], \"images\")\n",
    "    else:\n",
    "        pred_folder = os.path.join(pred_folder_base, \"images\")\n",
    "    pred_img_path = os.path.join(pred_folder, os.path.basename(img_path))\n",
    "    \n",
    "    predictions.append(pred_img_path)\n",
    "    results_list.append(results)  # keep results for plotting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec33b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_000123.jpg: 256x320 1 train, 46.8ms\n",
      "Speed: 1.2ms preprocess, 46.8ms inference, 0.9ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio21\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_000768.jpg: 256x320 2 buss, 1 car, 45.6ms\n",
      "Speed: 0.9ms preprocess, 45.6ms inference, 0.8ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio22\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_002427.jpg: 256x320 1 chair, 1 sofa, 42.1ms\n",
      "Speed: 0.6ms preprocess, 42.1ms inference, 0.9ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio23\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_003207.jpg: 224x320 1 bottle, 4 chairs, 5 diningtables, 1 person, 36.7ms\n",
      "Speed: 0.9ms preprocess, 36.7ms inference, 0.9ms postprocess per image at shape (1, 3, 224, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio24\u001b[0m\n",
      "\n",
      "image 1/1 C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\images\\2007_003286.jpg: 256x320 1 bus, 1 train, 37.3ms\n",
      "Speed: 0.8ms preprocess, 37.3ms inference, 0.8ms postprocess per image at shape (1, 3, 256, 320)\n",
      "Results saved to \u001b[1mC:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\yolo_predictions_demo\\demo_portfolio25\u001b[0m\n",
      "Saved side-by-side image: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\portfolio_predictions\\portfolio_01.png\n",
      "Saved side-by-side image: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\portfolio_predictions\\portfolio_02.png\n",
      "Saved side-by-side image: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\portfolio_predictions\\portfolio_03.png\n",
      "Saved side-by-side image: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\portfolio_predictions\\portfolio_04.png\n",
      "Saved side-by-side image: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\portfolio_predictions\\portfolio_05.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Predict and generate portfolio\n",
    "# --------------------------\n",
    "portfolio_dir = os.path.join(repo_root, \"portfolio_predictions\")\n",
    "os.makedirs(portfolio_dir, exist_ok=True)\n",
    "\n",
    "portfolio_val_images = val_images[:5]  # pick first 5\n",
    "portfolio_results = []\n",
    "\n",
    "for img_path in portfolio_val_images:\n",
    "    # Run YOLO prediction\n",
    "    res = model.predict(\n",
    "        source=img_path,\n",
    "        imgsz=320,\n",
    "        conf=0.1,\n",
    "        save=True,       # saves annotated image\n",
    "        project=output_dir,\n",
    "        name=\"demo_portfolio\"\n",
    "    )\n",
    "    portfolio_results.append(res[0])  # res is a list of Results\n",
    "\n",
    "# --------------------------\n",
    "# Display and save side-by-side\n",
    "# --------------------------\n",
    "for idx, (orig_path, res) in enumerate(zip(portfolio_val_images, portfolio_results), start=1):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(mpimg.imread(orig_path))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    # YOLO prediction (draw directly from Results object)\n",
    "    img_with_boxes = res.plot()  # NumPy array with bounding boxes\n",
    "    axes[1].imshow(img_with_boxes)\n",
    "    axes[1].set_title(\"YOLO Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    save_path = os.path.join(portfolio_dir, f\"portfolio_{idx:02d}.png\")\n",
    "    fig.savefig(save_path, bbox_inches='tight')\n",
    "    print(f\"Saved side-by-side image: {save_path}\")\n",
    "    \n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pascalvoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
