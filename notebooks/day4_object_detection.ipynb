{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f95699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Data handling / visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import yaml\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc138c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (11): Concat()\n",
      "      (12): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (14): Concat()\n",
      "      (15): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (17): Concat()\n",
      "      (18): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (20): Concat()\n",
      "      (21): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Setup device\n",
    "# --------------------------\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Load pre-trained YOLOv8 model\n",
    "# --------------------------\n",
    "# Nano YOLOv8 for CPU\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "print(\"Model loaded:\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7db607e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO dataset prepared and voc.yaml created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Dataset setup\n",
    "# --------------------------\n",
    "# Paths\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\")\n",
    "yolo_dataset_dir = os.path.join(repo_root, \"YOLO_VOC\")\n",
    "os.makedirs(yolo_dataset_dir, exist_ok=True)\n",
    "\n",
    "# VOC Classes\n",
    "VOC_CLASSES = [\n",
    "    'aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow',\n",
    "    'diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "\n",
    "# Function to convert bounding boxes\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1.0 / size[0]\n",
    "    dh = 1.0 / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    return x*dw, y*dh, w*dw, h*dh\n",
    "\n",
    "# Conversion function\n",
    "def voc_to_yolo(voc_root, year=\"2012\", split=\"train\", output_dir=\"YOLO_VOC\"):\n",
    "    img_dir = os.path.join(voc_root, f\"VOC{year}\", \"JPEGImages\")\n",
    "    ann_dir = os.path.join(voc_root, f\"VOC{year}\", \"Annotations\")\n",
    "    split_file = os.path.join(voc_root, f\"VOC{year}\", \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "\n",
    "    # Output dirs\n",
    "    txt_output_dir = os.path.join(output_dir, split, \"labels\")\n",
    "    img_output_dir = os.path.join(output_dir, split, \"images\")\n",
    "    os.makedirs(txt_output_dir, exist_ok=True)\n",
    "    os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "    # Read official split list\n",
    "    with open(split_file, \"r\") as f:\n",
    "        img_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for img_id in img_ids:\n",
    "        # Copy image\n",
    "        src_img = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
    "        dst_img = os.path.join(img_output_dir, f\"{img_id}.jpg\")\n",
    "        shutil.copy(src_img, dst_img)\n",
    "\n",
    "        # Convert annotation\n",
    "        xml_file = os.path.join(ann_dir, f\"{img_id}.xml\")\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        w = int(root.find(\"size/width\").text)\n",
    "        h = int(root.find(\"size/height\").text)\n",
    "\n",
    "        yolo_labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            cls_name = obj.find(\"name\").text\n",
    "            if cls_name not in VOC_CLASSES:\n",
    "                continue\n",
    "            cls_id = VOC_CLASSES.index(cls_name)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            x_center, y_center, bw, bh = convert_bbox((w, h), (xmin, xmax, ymin, ymax))\n",
    "            yolo_labels.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\")\n",
    "\n",
    "        txt_file_path = os.path.join(txt_output_dir, f\"{img_id}.txt\")\n",
    "        with open(txt_file_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_labels))\n",
    "\n",
    "\n",
    "# Convert train and val sets\n",
    "voc_to_yolo(voc_root, \"2012\", \"train\", yolo_dataset_dir)\n",
    "voc_to_yolo(voc_root, \"2012\", \"val\", yolo_dataset_dir)\n",
    "\n",
    "# Generate YAML config for YOLOv8\n",
    "voc_yaml = {\n",
    "    'train': os.path.join(yolo_dataset_dir, 'train', 'images'),\n",
    "    'val': os.path.join(yolo_dataset_dir, 'val', 'images'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(repo_root, \"voc.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(voc_yaml, f)\n",
    "print(\"YOLO dataset prepared and voc.yaml created at:\", yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4bddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train subset created with 800 images.\n",
      "val subset created with 200 images.\n",
      "Subset YAML created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Subset setup\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\", \"VOC2012\")\n",
    "subset_dir = os.path.join(repo_root, \"YOLO_VOC_subset\")\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "subset_sizes = {\n",
    "    \"train\": 800,   # number of images for CPU testing\n",
    "    \"val\": 200\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Function to create subset\n",
    "# --------------------------\n",
    "def create_yolo_subset(voc_root, original_dir, subset_dir, split, num_images):\n",
    "    split_file = os.path.join(voc_root, \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "    orig_images_dir = os.path.join(original_dir, split, \"images\")\n",
    "    orig_labels_dir = os.path.join(original_dir, split, \"labels\")\n",
    "\n",
    "    subset_images_dir = os.path.join(subset_dir, split, \"images\")\n",
    "    subset_labels_dir = os.path.join(subset_dir, split, \"labels\")\n",
    "    os.makedirs(subset_images_dir, exist_ok=True)\n",
    "    os.makedirs(subset_labels_dir, exist_ok=True)\n",
    "\n",
    "    # Load official split IDs\n",
    "    with open(split_file, \"r\") as f:\n",
    "        split_ids = [line.strip() + \".jpg\" for line in f.readlines()]\n",
    "\n",
    "    # Keep only those that exist in original_dir\n",
    "    available_images = [f for f in split_ids if f in os.listdir(orig_images_dir)]\n",
    "\n",
    "    # Random sample\n",
    "    sampled_images = random.sample(available_images, min(num_images, len(available_images)))\n",
    "\n",
    "    for img_file in sampled_images:\n",
    "        # Copy image\n",
    "        shutil.copy(os.path.join(orig_images_dir, img_file),\n",
    "                    os.path.join(subset_images_dir, img_file))\n",
    "        # Copy corresponding label\n",
    "        label_file = img_file.replace(\".jpg\", \".txt\")\n",
    "        shutil.copy(os.path.join(orig_labels_dir, label_file),\n",
    "                    os.path.join(subset_labels_dir, label_file))\n",
    "\n",
    "    print(f\"{split} subset created with {len(sampled_images)} images.\")\n",
    "\n",
    "# --------------------------\n",
    "# Create subsets\n",
    "# --------------------------\n",
    "for split in [\"train\", \"val\"]:\n",
    "    create_yolo_subset(voc_root, yolo_dataset_dir, subset_dir, split, subset_sizes[split])\n",
    "\n",
    "# --------------------------\n",
    "# Create subset YAML\n",
    "# --------------------------\n",
    "subset_yaml = {\n",
    "    'train': os.path.join(subset_dir, 'train'),\n",
    "    'val': os.path.join(subset_dir, 'val'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "subset_yaml_path = os.path.join(repo_root, \"voc_subset.yaml\")\n",
    "with open(subset_yaml_path, \"w\") as f:\n",
    "    yaml.dump(subset_yaml, f)\n",
    "print(\"Subset YAML created at:\", subset_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf68d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.194 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train11, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train11, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=20\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 210.0111.4 MB/s, size: 112.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels... 2812 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2812/2812 1485.8it/s 1.9s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 218.4132.7 MB/s, size: 76.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels... 698 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 698/698 1733.9it/s 0.4s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\train11\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train11\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G      1.195      3.357      1.152          7        224: 100% ━━━━━━━━━━━━ 703/703 3.3it/s 3:30<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 4.1it/s 21.4s0.2ss\n",
      "                   all        698       2015      0.581      0.341      0.367      0.259\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10         0G      1.261      2.203      1.191          6        224: 100% ━━━━━━━━━━━━ 703/703 3.3it/s 3:32<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 4.5it/s 19.4s0.2s\n",
      "                   all        698       2015       0.56      0.463      0.478       0.32\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10         0G      1.256      1.905      1.201          6        224: 100% ━━━━━━━━━━━━ 703/703 3.4it/s 3:30<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 4.2it/s 20.9s0.2ss\n",
      "                   all        698       2015      0.534      0.488      0.483      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10         0G      1.221       1.74      1.185          7        224: 100% ━━━━━━━━━━━━ 703/703 3.4it/s 3:28<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 4.1it/s 21.2s0.2ss\n",
      "                   all        698       2015      0.534       0.49      0.497      0.338\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10         0G      1.188      1.666      1.161          4        224: 100% ━━━━━━━━━━━━ 703/703 3.3it/s 3:35<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 4.0it/s 22.0s0.2ss\n",
      "                   all        698       2015      0.577      0.513      0.519      0.342\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10         0G      1.156      1.586      1.162          8        224: 100% ━━━━━━━━━━━━ 703/703 3.2it/s 3:40<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 4.2it/s 20.8s0.2ss\n",
      "                   all        698       2015      0.608      0.523      0.541      0.363\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10         0G       1.13      1.527      1.146         11        224: 100% ━━━━━━━━━━━━ 703/703 3.3it/s 3:31<0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 4.8it/s 18.4s0.2s\n",
      "                   all        698       2015      0.663      0.503      0.545      0.372\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10         0G      1.101      1.488      1.141          7        224: 100% ━━━━━━━━━━━━ 703/703 1.9it/s 6:11<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 2.4it/s 37.4s0.4ss\n",
      "                   all        698       2015       0.64      0.517      0.554      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10         0G       1.09      1.441      1.133          4        224: 100% ━━━━━━━━━━━━ 703/703 1.9it/s 6:14<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 2.6it/s 34.4s0.3ss\n",
      "                   all        698       2015      0.634      0.525      0.564      0.393\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10         0G      1.074      1.397      1.124          8        224: 100% ━━━━━━━━━━━━ 703/703 2.1it/s 5:30<0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 2.9it/s 30.5s0.3ss\n",
      "                   all        698       2015       0.64      0.522      0.561      0.395\n",
      "\n",
      "10 epochs completed in 0.781 hours.\n",
      "Optimizer stripped from runs\\detect\\train11\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train11\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train11\\weights\\best.pt...\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 88/88 2.8it/s 31.5s0.4ss\n",
      "                   all        698       2015       0.64      0.522      0.561      0.395\n",
      "             aeroplane         23         51      0.591      0.412      0.467      0.313\n",
      "               bicycle         33         41      0.574      0.493      0.551      0.415\n",
      "                  bird         38         90      0.795      0.333      0.408      0.263\n",
      "                  boat         26         40      0.418       0.55      0.477       0.28\n",
      "                bottle         43        101      0.539      0.347      0.353      0.207\n",
      "                   bus         35         48       0.71      0.771      0.798      0.653\n",
      "                   car         64        150      0.761      0.427      0.558      0.366\n",
      "                   cat         52         62      0.764      0.694      0.763      0.572\n",
      "                 chair         80        198      0.503      0.293      0.303      0.175\n",
      "                   cow         20         46      0.439      0.565      0.535      0.385\n",
      "           diningtable         35         37      0.541      0.784      0.627       0.38\n",
      "                   dog         82        101        0.7      0.531       0.63      0.462\n",
      "                 horse         26         52      0.659      0.404      0.478      0.381\n",
      "             motorbike         30         46      0.725      0.609      0.699      0.503\n",
      "                person        311        679      0.829      0.442      0.615      0.392\n",
      "           pottedplant         39         89      0.463       0.27      0.267      0.161\n",
      "                 sheep         22         67      0.641      0.427       0.56       0.36\n",
      "                  sofa         41         46       0.57      0.609      0.572      0.408\n",
      "                 train         34         37      0.929      0.838      0.919      0.768\n",
      "             tvmonitor         30         34      0.649      0.653      0.641      0.448\n",
      "Speed: 0.3ms preprocess, 33.8ms inference, 0.0ms loss, 1.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Fine-tune model (CPU-friendly)\n",
    "# --------------------------\n",
    "results = model.train(\n",
    "    data=subset_yaml_path,      #yaml_path for full dataset; subset_yaml_path for debugging/CPU\n",
    "    epochs=10,            # lower for CPU\n",
    "    batch=4,             # small batch for CPU\n",
    "    imgsz=224,           # smaller image size speeds up CPU training\n",
    "    device=device,        # CPU\n",
    "    freeze=10           #freezes first 10 layers (backbone)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6981a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 209.450.7 MB/s, size: 118.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache... 698 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 698/698 686591.0it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 175/175 11.7it/s 15.0s0.2s\n",
      "                   all        698       2015      0.567      0.582      0.569      0.412\n",
      "             aeroplane         23         51      0.597      0.435      0.464       0.35\n",
      "               bicycle         33         41       0.45      0.537      0.487       0.36\n",
      "                  bird         38         90      0.587      0.378      0.435      0.288\n",
      "                  boat         26         40      0.478      0.573      0.531      0.346\n",
      "                bottle         43        101      0.506      0.437      0.404      0.236\n",
      "                   bus         35         48      0.646      0.771      0.811      0.651\n",
      "                   car         64        150      0.784      0.509      0.681      0.454\n",
      "                   cat         52         62      0.757      0.726      0.793      0.636\n",
      "                 chair         80        198      0.394      0.384       0.29      0.187\n",
      "                   cow         20         46      0.532      0.652      0.582       0.43\n",
      "           diningtable         35         37      0.345      0.811      0.563       0.37\n",
      "                   dog         82        101      0.774      0.577      0.713       0.52\n",
      "                 horse         26         52      0.481      0.481      0.422      0.355\n",
      "             motorbike         30         46       0.58      0.674      0.589      0.444\n",
      "                person        311        679      0.793      0.446      0.657      0.454\n",
      "           pottedplant         39         89      0.447      0.337      0.311      0.184\n",
      "                 sheep         22         67      0.529      0.582       0.58       0.39\n",
      "                  sofa         41         46      0.348      0.783      0.596      0.407\n",
      "                 train         34         37      0.761      0.865       0.86      0.709\n",
      "             tvmonitor         30         34      0.542      0.676      0.606      0.476\n",
      "Speed: 0.2ms preprocess, 17.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train112\u001b[0m\n",
      "Overall mAP@0.5:0.95: 0.41224704301524817\n",
      "Overall Precision: 0.5665380450319126\n",
      "Overall Recall: 0.5816334946136991\n",
      "\n",
      "Per-class mAP:\n",
      "                class  mAP@0.5:0.95\n",
      "0          aeroplane      0.349917\n",
      "1            bicycle      0.359654\n",
      "2               bird      0.287867\n",
      "3               boat      0.346227\n",
      "4             bottle      0.235503\n",
      "5                bus      0.650968\n",
      "6                car      0.453805\n",
      "7                cat      0.636457\n",
      "8              chair      0.186551\n",
      "9                cow      0.429842\n",
      "10       diningtable      0.369715\n",
      "11               dog      0.519883\n",
      "12             horse      0.355454\n",
      "13         motorbike      0.443521\n",
      "14            person      0.454246\n",
      "15       pottedplant      0.184334\n",
      "16             sheep      0.389792\n",
      "17              sofa      0.406584\n",
      "18             train      0.709107\n",
      "19         tvmonitor      0.475512\n",
      "Overall      Overall      0.412247\n",
      "\n",
      "Per-class Precision:\n",
      "                class  Precision\n",
      "0          aeroplane   0.596613\n",
      "1            bicycle   0.449955\n",
      "2               bird   0.586583\n",
      "3               boat   0.478091\n",
      "4             bottle   0.506497\n",
      "5                bus   0.646062\n",
      "6                car   0.784387\n",
      "7                cat   0.756745\n",
      "8              chair   0.393905\n",
      "9                cow   0.531668\n",
      "10       diningtable   0.345441\n",
      "11               dog   0.774277\n",
      "12             horse   0.481039\n",
      "13         motorbike   0.579650\n",
      "14            person   0.792997\n",
      "15       pottedplant   0.447061\n",
      "16             sheep   0.528729\n",
      "17              sofa   0.348167\n",
      "18             train   0.761362\n",
      "19         tvmonitor   0.541533\n",
      "Overall      Overall   0.566538\n",
      "\n",
      "Per-class Recall:\n",
      "                class    Recall\n",
      "0          aeroplane  0.435082\n",
      "1            bicycle  0.536585\n",
      "2               bird  0.377778\n",
      "3               boat  0.572573\n",
      "4             bottle  0.436966\n",
      "5                bus  0.770833\n",
      "6                car  0.509328\n",
      "7                cat  0.725806\n",
      "8              chair  0.383838\n",
      "9                cow  0.652174\n",
      "10       diningtable  0.810811\n",
      "11               dog  0.577389\n",
      "12             horse  0.480769\n",
      "13         motorbike  0.673913\n",
      "14            person  0.445711\n",
      "15       pottedplant  0.337079\n",
      "16             sheep  0.582090\n",
      "17              sofa  0.782609\n",
      "18             train  0.864865\n",
      "19         tvmonitor  0.676471\n",
      "Overall      Overall  0.581633\n",
      "\n",
      "Per-class F1:\n",
      "           class        F1\n",
      "0     aeroplane  0.503201\n",
      "1       bicycle  0.489466\n",
      "2          bird  0.459574\n",
      "3          boat  0.521083\n",
      "4        bottle  0.469169\n",
      "5           bus  0.702954\n",
      "6           car  0.617617\n",
      "7           cat  0.740953\n",
      "8         chair  0.388806\n",
      "9           cow  0.585787\n",
      "10  diningtable  0.484474\n",
      "11          dog  0.661493\n",
      "12        horse  0.480904\n",
      "13    motorbike  0.623237\n",
      "14       person  0.570671\n",
      "15  pottedplant  0.384356\n",
      "16        sheep  0.554127\n",
      "17         sofa  0.481931\n",
      "18        train  0.809819\n",
      "19    tvmonitor  0.601527\n",
      "20      Overall  0.573986\n",
      "All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Compute full validation metrics\n",
    "# --------------------------\n",
    "metrics = model.val(\n",
    "    data=subset_yaml_path,\n",
    "    split=\"val\",\n",
    "    imgsz=320,\n",
    "    conf=0.1\n",
    ")\n",
    "\n",
    "print(\"Overall mAP@0.5:0.95:\", metrics.box.map)\n",
    "print(\"Overall Precision:\", metrics.box.mp)\n",
    "print(\"Overall Recall:\", metrics.box.mr)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 6. Export per-class metrics into DataFrames\n",
    "# --------------------------\n",
    "\n",
    "# mAP DataFrame\n",
    "df_map = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"mAP@0.5:0.95\": metrics.box.maps\n",
    "})\n",
    "df_map.loc[\"Overall\"] = [\"Overall\", metrics.box.map]\n",
    "\n",
    "# Precision DataFrame\n",
    "df_precision = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Precision\": metrics.box.p\n",
    "})\n",
    "df_precision.loc[\"Overall\"] = [\"Overall\", metrics.box.mp]\n",
    "\n",
    "# Recall DataFrame\n",
    "df_recall = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Recall\": metrics.box.r\n",
    "})\n",
    "df_recall.loc[\"Overall\"] = [\"Overall\", metrics.box.mr]\n",
    "\n",
    "# F1 DataFrame\n",
    "f1_per_class = 2 * (metrics.box.p * metrics.box.r) / (metrics.box.p + metrics.box.r + 1e-6)\n",
    "overall_f1 = 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr + 1e-6)\n",
    "df_f1 = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"F1\": f1_per_class\n",
    "})\n",
    "df_f1.loc[len(df_f1)] = [\"Overall\", overall_f1]\n",
    "\n",
    "print(\"\\nPer-class mAP:\\n\", df_map)\n",
    "print(\"\\nPer-class Precision:\\n\", df_precision)\n",
    "print(\"\\nPer-class Recall:\\n\", df_recall)\n",
    "print(\"\\nPer-class F1:\\n\", df_f1)\n",
    "\n",
    "# Save to CSVs\n",
    "df_map.to_csv(os.path.join(repo_root, \"yolo_val_map.csv\"), index=False)\n",
    "df_precision.to_csv(os.path.join(repo_root, \"yolo_val_precision.csv\"), index=False)\n",
    "df_recall.to_csv(os.path.join(repo_root, \"yolo_val_recall.csv\"), index=False)\n",
    "df_f1.to_csv(os.path.join(repo_root, \"yolo_val_f1.csv\"), index=False)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Generate per-class bar charts\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "# mAP bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_map[\"class\"][:-1], df_map[\"mAP@0.5:0.95\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"mAP@0.5:0.95\")\n",
    "plt.title(\"YOLOv8 Per-Class mAP (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_map.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Precision bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_precision[\"class\"][:-1], df_precision[\"Precision\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"YOLOv8 Per-Class Precision (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_precision.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Recall bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_recall[\"class\"][:-1], df_recall[\"Recall\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"YOLOv8 Per-Class Recall (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_recall.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# F1 bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_f1[\"class\"][:-1], df_f1[\"F1\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"YOLOv8 Per-Class F1 (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_f1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# --------------------------\n",
    "# 8. Combined grouped bar chart (Precision, Recall, F1)\n",
    "# --------------------------\n",
    "classes = df_precision[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))  # positions\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width, df_precision[\"Precision\"][:-1], width=width, label=\"Precision\")\n",
    "plt.bar(x, df_recall[\"Recall\"][:-1], width=width, label=\"Recall\")\n",
    "plt.bar(x + width, df_f1[\"F1\"][:-1], width=width, label=\"F1\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Precision, Recall, and F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_prf1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba92ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated scatterplot: Per-class mAP vs F1 (with correlation)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Scatterplot: Per-class mAP vs F1\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "\n",
    "# Exclude \"Overall\" row\n",
    "perclass_map = df_map[\"mAP@0.5:0.95\"][:-1]\n",
    "perclass_f1 = df_f1[\"F1\"][:-1]\n",
    "classes = df_map[\"class\"][:-1]\n",
    "\n",
    "# Compute correlation\n",
    "corr, pval = pearsonr(perclass_map, perclass_f1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=perclass_map, y=perclass_f1)\n",
    "\n",
    "# Annotate each class\n",
    "for i, cls in enumerate(classes):\n",
    "    plt.text(perclass_map.iloc[i] + 0.005,\n",
    "             perclass_f1.iloc[i] + 0.005,\n",
    "             cls, fontsize=8)\n",
    "\n",
    "plt.xlabel(\"mAP@0.5:0.95\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(f\"YOLOv8 Per-Class mAP vs F1 (Validation)\\nPearson r={corr:.2f}, p={pval:.3f}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_map_vs_f1_scatter.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated scatterplot: Per-class mAP vs F1 (with correlation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f972399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated grouped bar chart: Per-class mAP vs F1\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Compare per-class mAP vs F1\n",
    "# --------------------------\n",
    "\n",
    "\n",
    "classes = df_map[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35  # bar width\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width/2, df_map[\"mAP@0.5:0.95\"][:-1], width=width, label=\"mAP@0.5:0.95\")\n",
    "plt.bar(x + width/2, df_f1[\"F1\"][:-1], width=width, label=\"F1 Score\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Comparison: mAP vs F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_map_vs_f1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated grouped bar chart: Per-class mAP vs F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4910bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_01.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_02.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_03.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_04.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_05.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_06.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_07.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_08.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_09.png\n",
      "Saved GT vs Prediction comparison: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\comparison_predictions\\comparison_10.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 8. Side-by-side GT vs Predictions (10 random val images)\n",
    "# --------------------------\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "val_images_dir = os.path.join(subset_dir, \"val\", \"images\")\n",
    "val_labels_dir = os.path.join(subset_dir, \"val\", \"labels\")\n",
    "\n",
    "# Randomly select 10 val images\n",
    "all_val_images = glob.glob(os.path.join(val_images_dir, \"*.jpg\"))\n",
    "sample_val_images = random.sample(all_val_images, min(10, len(all_val_images)))\n",
    "\n",
    "comparison_dir = os.path.join(repo_root, \"comparison_predictions\")\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "for idx, img_path in enumerate(sample_val_images, start=1):\n",
    "    # Load GT labels\n",
    "    label_path = os.path.join(val_labels_dir, os.path.basename(img_path).replace(\".jpg\", \".txt\"))\n",
    "    gt_boxes = []\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                cls_id, x, y, w, h = line.strip().split()\n",
    "                cls_id = int(cls_id)\n",
    "                gt_boxes.append((VOC_CLASSES[cls_id], float(x), float(y), float(w), float(h)))\n",
    "\n",
    "    # Run YOLO prediction\n",
    "    results = model.predict(source=img_path, imgsz=320, conf=0.1, verbose=False)\n",
    "    res = results[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Left: Ground truth (draw bounding boxes manually)\n",
    "    img = mpimg.imread(img_path)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    h, w = img.shape[:2]\n",
    "    for cls_name, x, y, bw, bh in gt_boxes:\n",
    "        xmin = int((x - bw/2) * w)\n",
    "        ymin = int((y - bh/2) * h)\n",
    "        xmax = int((x + bw/2) * w)\n",
    "        ymax = int((y + bh/2) * h)\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin,\n",
    "                             linewidth=2, edgecolor=\"lime\", facecolor=\"none\")\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].text(xmin, ymin-5, cls_name, color=\"lime\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "    # Right: YOLO predictions (ultralytics res.plot())\n",
    "    img_pred = res.plot()\n",
    "    axes[1].imshow(img_pred)\n",
    "    axes[1].set_title(\"YOLO Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_dir, f\"comparison_{idx:02d}.png\")\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved GT vs Prediction comparison: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: (21, 2)\n",
      "       class  Precision    Recall        F1\n",
      "0  aeroplane   0.500000  0.250000  0.333333\n",
      "1    bicycle   1.000000  0.250000  0.400000\n",
      "2       bird   0.800000  0.500000  0.615385\n",
      "3        car   0.600000  0.315789  0.413793\n",
      "4        cat   0.916667  0.687500  0.785714\n",
      "5      chair   0.538462  0.411765  0.466667\n",
      "6      horse   1.000000  0.250000  0.400000\n",
      "7  motorbike   1.000000  0.125000  0.222222\n",
      "8     person   0.835821  0.910569  0.871595\n",
      "9  tvmonitor   0.500000  0.571429  0.533333\n",
      "       class  Precision    Recall        F1\n",
      "0  aeroplane   0.596613  0.435082  0.503201\n",
      "1    bicycle   0.449955  0.536585  0.489466\n",
      "2       bird   0.586583  0.377778  0.459574\n",
      "3        car   0.784387  0.509328  0.617617\n",
      "4        cat   0.756745  0.725806  0.740953\n",
      "5      chair   0.393905  0.383838  0.388806\n",
      "6      horse   0.481039  0.480769  0.480904\n",
      "7  motorbike   0.579650  0.673913  0.623237\n",
      "8     person   0.792997  0.445711  0.570671\n",
      "9  tvmonitor   0.541533  0.676471  0.601527\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 9. Comparing YOLO and ResNet's classification performance on Pascal VOC\n",
    "# --------------------------\n",
    "# \n",
    "# # --------------------------\n",
    "# Paths to CSV files\n",
    "# --------------------------\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "\n",
    "resnet_csv = os.path.join(repo_root, \"resnet_val_precision_recall_f1.csv\")\n",
    "yolo_precision_csv = os.path.join(repo_root, \"yolo_val_precision.csv\")\n",
    "yolo_recall_csv = os.path.join(repo_root, \"yolo_val_recall.csv\")\n",
    "yolo_f1_csv = os.path.join(repo_root, \"yolo_val_f1.csv\")\n",
    "\n",
    "# --------------------------\n",
    "# Read CSVs\n",
    "# --------------------------\n",
    "df_resnet = pd.read_csv(resnet_csv)\n",
    "df_yolo_precision = pd.read_csv(yolo_precision_csv)\n",
    "df_yolo_recall = pd.read_csv(yolo_recall_csv)\n",
    "df_yolo_f1 = pd.read_csv(yolo_f1_csv)\n",
    "\n",
    "print(\"1:\",df_yolo_precision.shape)\n",
    "#print(\"1:\",df_yolo_recall)\n",
    "#(\"1:\",df_yolo_f1)\n",
    "# --------------------------\n",
    "# Clean class names (strip whitespace)\n",
    "# --------------------------\n",
    "df_resnet['class'] = df_resnet['class'].str.strip()\n",
    "df_yolo_precision['class'] = df_yolo_precision['class'].str.strip()\n",
    "df_yolo_recall['class'] = df_yolo_recall['class'].str.strip()\n",
    "df_yolo_f1['class'] = df_yolo_f1['class'].str.strip()\n",
    "\n",
    "# --------------------------\n",
    "# Filter ResNet rows to exclude Macro/Micro averages\n",
    "# --------------------------\n",
    "df_resnet_filtered = df_resnet[~df_resnet['class'].isin(['Macro Avg', 'Micro Avg'])].copy()\n",
    "\n",
    "# Identify valid classes (non-zero metrics)\n",
    "valid_classes = df_resnet_filtered[\n",
    "    (df_resnet_filtered['Precision'] > 0) | \n",
    "    (df_resnet_filtered['Recall'] > 0) | \n",
    "    (df_resnet_filtered['F1'] > 0)\n",
    "]['class'].tolist()\n",
    "\n",
    "# Filter ResNet to only valid classes\n",
    "df_resnet = df_resnet_filtered[df_resnet_filtered['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "\n",
    "# Filter YOLO CSVs to match valid classes\n",
    "df_yolo_precision = df_yolo_precision[df_yolo_precision['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "df_yolo_recall = df_yolo_recall[df_yolo_recall['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "df_yolo_f1 = df_yolo_f1[df_yolo_f1['class'].isin(valid_classes)].reset_index(drop=True)\n",
    "\n",
    "# Merge into a single YOLO DataFrame\n",
    "df_yolo = pd.DataFrame({\"class\": valid_classes})\n",
    "df_yolo = df_yolo.merge(df_yolo_precision[['class', 'Precision']], on='class', how='left')\n",
    "df_yolo = df_yolo.merge(df_yolo_recall[['class', 'Recall']], on='class', how='left')\n",
    "df_yolo = df_yolo.merge(df_yolo_f1[['class', 'F1']], on='class', how='left')\n",
    "df_yolo.fillna(0, inplace=True)\n",
    "\n",
    "print(df_resnet)\n",
    "print(df_yolo)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e171e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --------------------------\n",
    "# Function to create side-by-side bar charts\n",
    "# --------------------------\n",
    "def plot_metric_comparison(df_resnet, df_yolo, metric, save_path=None):\n",
    "    x = np.arange(len(df_resnet))  # positions for classes\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,6))\n",
    "    ax.bar(x - width/2, df_resnet[metric], width, label='ResNet', color='skyblue')\n",
    "    ax.bar(x + width/2, df_yolo[metric], width, label='YOLO', color='orange')\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(df_resnet['class'], rotation=45, ha='right')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'Per-Class {metric} Comparison: YOLO vs ResNet')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# Generate bar charts for Precision, Recall, F1\n",
    "# --------------------------\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"Precision\", os.path.join(repo_root, \"comparison_precision.png\"))\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"Recall\", os.path.join(repo_root, \"comparison_recall.png\"))\n",
    "plot_metric_comparison(df_resnet, df_yolo, \"F1\", os.path.join(repo_root, \"comparison_f1.png\"))\n",
    "\n",
    "# --------------------------\n",
    "# Additional recommended graphic: grouped bars for all metrics\n",
    "# --------------------------\n",
    "metrics = ['Precision', 'Recall', 'F1']\n",
    "x = np.arange(len(df_resnet))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14,7))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + (i-1)*width, df_resnet[metric], width, label=f'ResNet {metric}', alpha=0.8)\n",
    "    ax.bar(x + (i-1)*width, df_yolo[metric], width, label=f'YOLO {metric}', alpha=0.5, edgecolor='black', fill=False)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_resnet['class'], rotation=45, ha='right')\n",
    "ax.set_ylabel(\"Metric Value\")\n",
    "ax.set_title(\"Per-Class Metrics Comparison: YOLO vs ResNet\")\n",
    "ax.legend(fontsize='small', ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"comparison_grouped_metrics.png\"), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "851ac541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Grouped bar chart with 6 categories\n",
    "# --------------------------\n",
    "metrics = ['Precision', 'Recall', 'F1']\n",
    "x = np.arange(len(df_resnet))\n",
    "width = 0.12\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,7))\n",
    "\n",
    "# Color-blind friendly palette (Okabe-Ito)\n",
    "metric_colors = {\n",
    "    'Precision': '#0072B2',  # blue\n",
    "    'Recall': '#009E73',     # green\n",
    "    'F1': '#E69F00'          # orange\n",
    "}\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # ResNet\n",
    "    bars_resnet = ax.bar(x + (i-1)*2*width, df_resnet[metric], width,\n",
    "                         color=metric_colors[metric], alpha=0.9)\n",
    "    handles.append(bars_resnet)\n",
    "    labels.append(f\"ResNet {metric}\")\n",
    "\n",
    "    # YOLO\n",
    "    bars_yolo = ax.bar(x + (i-1)*2*width + width, df_yolo[metric], width,\n",
    "                       color=metric_colors[metric], alpha=0.6, hatch='//', edgecolor='black')\n",
    "    handles.append(bars_yolo)\n",
    "    labels.append(f\"YOLO {metric}\")\n",
    "\n",
    "# Formatting\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df_resnet['class'], rotation=45, ha='right', fontsize=10)\n",
    "ax.set_ylabel(\"Metric Value\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_title(\"Per-Class Precision, Recall, F1: YOLO vs ResNet\", fontsize=14, pad=15)\n",
    "\n",
    "# Full legend\n",
    "ax.legend(handles, labels, fontsize=10, loc='upper right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"comparison_grouped_metrics_clean.png\"), bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "35a422b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\precision_recall_scatter.png\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Scatter plot: Precision vs Recall\n",
    "# --------------------------\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# ResNet points (blue circles)\n",
    "plt.scatter(df_resnet['Precision'], df_resnet['Recall'], \n",
    "            color=\"#36454F\", marker=\"o\", s=100, label=\"ResNet\")\n",
    "\n",
    "# YOLO points (orange triangles)\n",
    "plt.scatter(df_yolo_precision['Precision'], df_yolo_recall['Recall'], \n",
    "            color=\"#CC79A7\", marker=\"^\", s=100, label=\"YOLO\")\n",
    "\n",
    "# Add class labels with larger font\n",
    "for i, cls in enumerate(df_resnet['class']):\n",
    "    plt.text(df_resnet['Precision'][i] + 0.015, df_resnet['Recall'][i] + 0.015, cls,\n",
    "             fontsize=10, color=\"#36454F\")\n",
    "    plt.text(df_yolo_precision['Precision'][i] + 0.015, df_yolo_recall['Recall'][i] - 0.025, cls,\n",
    "             fontsize=10, color=\"#CC79A7\")\n",
    "\n",
    "# Labels and formatting\n",
    "plt.xlabel(\"Precision\", fontsize=12)\n",
    "plt.ylabel(\"Recall\", fontsize=12)\n",
    "plt.title(\"Precision vs Recall per Class: YOLO vs ResNet\", fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Set axis limits with small margins so points don't get cut off\n",
    "plt.xlim(0.08, 1.02)\n",
    "plt.ylim(0.08, 1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "save_path = os.path.join(repo_root, \"precision_recall_scatter.png\")\n",
    "plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"Scatter plot saved to {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pascalvoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
