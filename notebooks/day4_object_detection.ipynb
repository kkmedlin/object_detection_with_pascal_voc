{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f95699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# YOLO\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Data handling / visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import yaml\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc138c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model loaded: YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0-1): 2 x Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (11): Concat()\n",
      "      (12): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (14): Concat()\n",
      "      (15): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (17): Concat()\n",
      "      (18): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (20): Concat()\n",
      "      (21): C2f(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 1. Setup device\n",
    "# --------------------------\n",
    "device = 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Load pre-trained YOLOv8 model\n",
    "# --------------------------\n",
    "# Nano YOLOv8 for CPU\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "print(\"Model loaded:\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db607e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO dataset prepared and voc.yaml created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Dataset setup\n",
    "# --------------------------\n",
    "# Paths\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\")\n",
    "yolo_dataset_dir = os.path.join(repo_root, \"YOLO_VOC\")\n",
    "os.makedirs(yolo_dataset_dir, exist_ok=True)\n",
    "\n",
    "# VOC Classes\n",
    "VOC_CLASSES = [\n",
    "    'aeroplane','bicycle','bird','boat','bottle','bus','car','cat','chair','cow',\n",
    "    'diningtable','dog','horse','motorbike','person','pottedplant','sheep','sofa','train','tvmonitor'\n",
    "]\n",
    "\n",
    "# Function to convert bounding boxes\n",
    "def convert_bbox(size, box):\n",
    "    dw = 1.0 / size[0]\n",
    "    dh = 1.0 / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    return x*dw, y*dh, w*dw, h*dh\n",
    "\n",
    "# Conversion function\n",
    "def voc_to_yolo(voc_root, year=\"2012\", split=\"train\", output_dir=\"YOLO_VOC\"):\n",
    "    img_dir = os.path.join(voc_root, f\"VOC{year}\", \"JPEGImages\")\n",
    "    ann_dir = os.path.join(voc_root, f\"VOC{year}\", \"Annotations\")\n",
    "    split_file = os.path.join(voc_root, f\"VOC{year}\", \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "\n",
    "    # Output dirs\n",
    "    txt_output_dir = os.path.join(output_dir, split, \"labels\")\n",
    "    img_output_dir = os.path.join(output_dir, split, \"images\")\n",
    "    os.makedirs(txt_output_dir, exist_ok=True)\n",
    "    os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "    # Read official split list\n",
    "    with open(split_file, \"r\") as f:\n",
    "        img_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for img_id in img_ids:\n",
    "        # Copy image\n",
    "        src_img = os.path.join(img_dir, f\"{img_id}.jpg\")\n",
    "        dst_img = os.path.join(img_output_dir, f\"{img_id}.jpg\")\n",
    "        shutil.copy(src_img, dst_img)\n",
    "\n",
    "        # Convert annotation\n",
    "        xml_file = os.path.join(ann_dir, f\"{img_id}.xml\")\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        w = int(root.find(\"size/width\").text)\n",
    "        h = int(root.find(\"size/height\").text)\n",
    "\n",
    "        yolo_labels = []\n",
    "        for obj in root.findall(\"object\"):\n",
    "            cls_name = obj.find(\"name\").text\n",
    "            if cls_name not in VOC_CLASSES:\n",
    "                continue\n",
    "            cls_id = VOC_CLASSES.index(cls_name)\n",
    "            bbox = obj.find(\"bndbox\")\n",
    "            xmin = float(bbox.find(\"xmin\").text)\n",
    "            ymin = float(bbox.find(\"ymin\").text)\n",
    "            xmax = float(bbox.find(\"xmax\").text)\n",
    "            ymax = float(bbox.find(\"ymax\").text)\n",
    "            x_center, y_center, bw, bh = convert_bbox((w, h), (xmin, xmax, ymin, ymax))\n",
    "            yolo_labels.append(f\"{cls_id} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\")\n",
    "\n",
    "        txt_file_path = os.path.join(txt_output_dir, f\"{img_id}.txt\")\n",
    "        with open(txt_file_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_labels))\n",
    "\n",
    "\n",
    "# Convert train and val sets\n",
    "voc_to_yolo(voc_root, \"2012\", \"train\", yolo_dataset_dir)\n",
    "voc_to_yolo(voc_root, \"2012\", \"val\", yolo_dataset_dir)\n",
    "\n",
    "# Generate YAML config for YOLOv8\n",
    "voc_yaml = {\n",
    "    'train': os.path.join(yolo_dataset_dir, 'train', 'images'),\n",
    "    'val': os.path.join(yolo_dataset_dir, 'val', 'images'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(repo_root, \"voc.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    yaml.dump(voc_yaml, f)\n",
    "print(\"YOLO dataset prepared and voc.yaml created at:\", yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4bddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train subset created with 800 images.\n",
      "val subset created with 200 images.\n",
      "Subset YAML created at: C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 3. Subset setup\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# Paths\n",
    "# --------------------------\n",
    "repo_root = \"C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\"\n",
    "voc_root = os.path.join(repo_root, \"VOCdevkit\", \"VOC2012\")\n",
    "subset_dir = os.path.join(repo_root, \"YOLO_VOC_subset\")\n",
    "os.makedirs(subset_dir, exist_ok=True)\n",
    "\n",
    "subset_sizes = {\n",
    "    \"train\": 800,   # number of images for CPU testing\n",
    "    \"val\": 200\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# Function to create subset\n",
    "# --------------------------\n",
    "def create_yolo_subset(voc_root, original_dir, subset_dir, split, num_images):\n",
    "    split_file = os.path.join(voc_root, \"ImageSets\", \"Main\", f\"{split}.txt\")\n",
    "    orig_images_dir = os.path.join(original_dir, split, \"images\")\n",
    "    orig_labels_dir = os.path.join(original_dir, split, \"labels\")\n",
    "\n",
    "    subset_images_dir = os.path.join(subset_dir, split, \"images\")\n",
    "    subset_labels_dir = os.path.join(subset_dir, split, \"labels\")\n",
    "    os.makedirs(subset_images_dir, exist_ok=True)\n",
    "    os.makedirs(subset_labels_dir, exist_ok=True)\n",
    "\n",
    "    # Load official split IDs\n",
    "    with open(split_file, \"r\") as f:\n",
    "        split_ids = [line.strip() + \".jpg\" for line in f.readlines()]\n",
    "\n",
    "    # Keep only those that exist in original_dir\n",
    "    available_images = [f for f in split_ids if f in os.listdir(orig_images_dir)]\n",
    "\n",
    "    # Random sample\n",
    "    sampled_images = random.sample(available_images, min(num_images, len(available_images)))\n",
    "\n",
    "    for img_file in sampled_images:\n",
    "        # Copy image\n",
    "        shutil.copy(os.path.join(orig_images_dir, img_file),\n",
    "                    os.path.join(subset_images_dir, img_file))\n",
    "        # Copy corresponding label\n",
    "        label_file = img_file.replace(\".jpg\", \".txt\")\n",
    "        shutil.copy(os.path.join(orig_labels_dir, label_file),\n",
    "                    os.path.join(subset_labels_dir, label_file))\n",
    "\n",
    "    print(f\"{split} subset created with {len(sampled_images)} images.\")\n",
    "\n",
    "# --------------------------\n",
    "# Create subsets\n",
    "# --------------------------\n",
    "for split in [\"train\", \"val\"]:\n",
    "    create_yolo_subset(voc_root, yolo_dataset_dir, subset_dir, split, subset_sizes[split])\n",
    "\n",
    "# --------------------------\n",
    "# Create subset YAML\n",
    "# --------------------------\n",
    "subset_yaml = {\n",
    "    'train': os.path.join(subset_dir, 'train'),\n",
    "    'val': os.path.join(subset_dir, 'val'),\n",
    "    'nc': len(VOC_CLASSES),\n",
    "    'names': VOC_CLASSES\n",
    "}\n",
    "\n",
    "subset_yaml_path = os.path.join(repo_root, \"voc_subset.yaml\")\n",
    "with open(subset_yaml_path, \"w\") as f:\n",
    "    yaml.dump(subset_yaml, f)\n",
    "print(\"Subset YAML created at:\", subset_yaml_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf68d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.193 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=4, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:/Users/kamed/Desktop/argonne_K/object_detection_with_pascal_voc\\voc_subset.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=10, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train102, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\detect\\train102, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=0, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755212  ultralytics.nn.modules.head.Detect           [20, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,014,748 parameters, 3,014,732 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 569.9260.2 MB/s, size: 89.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels... 2225 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2225/2225 1848.9it/s 1.2s0.1s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 724.4312.6 MB/s, size: 103.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels... 509 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 509/509 2097.6it/s 0.2s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache\n",
      "Plotting labels to runs\\detect\\train102\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000417, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train102\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10         0G      1.146      1.552      1.147          5        224: 100% ━━━━━━━━━━━━ 557/557 4.3it/s 2:09<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 5.4it/s 11.9s0.2s\n",
      "                   all        509       1425      0.592      0.454      0.492      0.335\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10         0G      1.157      1.581       1.14         30        224: 100% ━━━━━━━━━━━━ 557/557 4.1it/s 2:15<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.4it/s 14.6s0.2s\n",
      "                   all        509       1425      0.583      0.464      0.487      0.331\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10         0G      1.164      1.559      1.155          3        224: 100% ━━━━━━━━━━━━ 557/557 3.4it/s 2:42<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.5it/s 14.3s0.2s\n",
      "                   all        509       1425      0.596       0.46      0.507      0.343\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10         0G      1.168      1.519      1.155          1        224: 100% ━━━━━━━━━━━━ 557/557 3.6it/s 2:35<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.4it/s 14.6s0.2s\n",
      "                   all        509       1425      0.582      0.477      0.496      0.338\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10         0G       1.13      1.488       1.14          1        224: 100% ━━━━━━━━━━━━ 557/557 3.6it/s 2:37<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.4it/s 14.5s0.2s\n",
      "                   all        509       1425      0.523      0.509      0.492      0.334\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10         0G      1.112      1.441      1.128          4        224: 100% ━━━━━━━━━━━━ 557/557 3.4it/s 2:42<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.7it/s 13.6s0.2s\n",
      "                   all        509       1425      0.562      0.513      0.525      0.364\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10         0G      1.095      1.401      1.119          5        224: 100% ━━━━━━━━━━━━ 557/557 3.7it/s 2:31<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.6it/s 14.0s0.2s\n",
      "                   all        509       1425      0.595      0.518      0.523       0.36\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10         0G      1.081      1.365      1.114          2        224: 100% ━━━━━━━━━━━━ 557/557 3.4it/s 2:44<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.3it/s 14.9s0.2s\n",
      "                   all        509       1425        0.6        0.5      0.511      0.353\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10         0G      1.061      1.335      1.109          1        224: 100% ━━━━━━━━━━━━ 557/557 3.6it/s 2:37<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.3it/s 14.7s0.2s\n",
      "                   all        509       1425      0.605      0.507      0.529      0.361\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10         0G      1.041      1.302      1.094          2        224: 100% ━━━━━━━━━━━━ 557/557 3.4it/s 2:46<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 3.9it/s 16.3s0.2s\n",
      "                   all        509       1425      0.642       0.49      0.532      0.366\n",
      "\n",
      "10 epochs completed in 0.468 hours.\n",
      "Optimizer stripped from runs\\detect\\train102\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from runs\\detect\\train102\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating runs\\detect\\train102\\weights\\best.pt...\n",
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "Model summary (fused): 72 layers, 3,009,548 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 64/64 4.1it/s 15.5s0.2s\n",
      "                   all        509       1425      0.642       0.49      0.532      0.366\n",
      "             aeroplane         15         37        0.6       0.27      0.313      0.228\n",
      "               bicycle         22         28       0.56      0.464      0.567      0.371\n",
      "                  bird         29         79      0.833      0.316      0.384      0.259\n",
      "                  boat         18         29      0.372      0.655      0.528       0.27\n",
      "                bottle         24         74      0.584      0.351      0.349      0.188\n",
      "                   bus         26         37      0.868      0.703      0.792      0.653\n",
      "                   car         44         98      0.939      0.312      0.533      0.344\n",
      "                   cat         40         49      0.843      0.656      0.787      0.502\n",
      "                 chair         54        130      0.483        0.3      0.294      0.172\n",
      "                   cow         10         27      0.358      0.481      0.453      0.306\n",
      "           diningtable         23         24      0.469      0.792      0.667      0.464\n",
      "                   dog         62         76       0.79      0.496       0.64      0.498\n",
      "                 horse         19         43      0.775      0.302       0.42      0.338\n",
      "             motorbike         15         18      0.401      0.667       0.57      0.438\n",
      "                person        228        473      0.832      0.408      0.619       0.41\n",
      "           pottedplant         28         73      0.447      0.266      0.222       0.13\n",
      "                 sheep         16         54      0.737      0.363      0.514      0.294\n",
      "                  sofa         29         32      0.482      0.594      0.559      0.384\n",
      "                 train         25         27      0.992      0.852      0.906      0.737\n",
      "             tvmonitor         14         17      0.481      0.545      0.518      0.338\n",
      "Speed: 0.2ms preprocess, 20.5ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train102\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 4. Fine-tune model (CPU-friendly)\n",
    "# --------------------------\n",
    "results = model.train(\n",
    "    data=subset_yaml_path,      #yaml_path for full dataset; subset_yaml_path for debugging/CPU\n",
    "    epochs=10,            # lower for CPU\n",
    "    batch=4,             # small batch for CPU\n",
    "    imgsz=224,           # smaller image size speeds up CPU training\n",
    "    device=device,        # CPU\n",
    "    freeze=10           #freezes first 10 layers (backbone)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6981a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.191  Python-3.10.18 torch-2.8.0+cpu CPU (12th Gen Intel Core(TM) i7-1260P)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 649.8437.6 MB/s, size: 94.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kamed\\Desktop\\argonne_K\\object_detection_with_pascal_voc\\YOLO_VOC_subset\\val\\labels.cache... 509 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 509/509 509036.9it/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 128/128 9.2it/s 14.0s0.1s\n",
      "                   all        509       1425       0.51      0.577      0.528      0.376\n",
      "             aeroplane         15         37      0.595      0.318      0.353      0.265\n",
      "               bicycle         22         28      0.473      0.643      0.521      0.403\n",
      "                  bird         29         79      0.533      0.367      0.358      0.247\n",
      "                  boat         18         29      0.478      0.759      0.529      0.342\n",
      "                bottle         24         74      0.441      0.486      0.385      0.195\n",
      "                   bus         26         37      0.728      0.784      0.819      0.619\n",
      "                   car         44         98      0.699       0.51      0.627      0.416\n",
      "                   cat         40         49      0.707      0.788      0.775      0.574\n",
      "                 chair         54        130      0.329      0.392      0.255      0.169\n",
      "                   cow         10         27      0.502       0.56      0.508      0.334\n",
      "           diningtable         23         24      0.277      0.708      0.497      0.327\n",
      "                   dog         62         76      0.694      0.598      0.665      0.535\n",
      "                 horse         19         43      0.444      0.326       0.38      0.306\n",
      "             motorbike         15         18      0.324      0.667      0.588      0.436\n",
      "                person        228        473      0.786      0.518      0.663      0.463\n",
      "           pottedplant         28         73      0.366      0.301      0.241      0.147\n",
      "                 sheep         16         54      0.472      0.593      0.538       0.35\n",
      "                  sofa         29         32      0.237      0.688      0.476      0.283\n",
      "                 train         25         27      0.587      0.889      0.796      0.643\n",
      "             tvmonitor         14         17      0.526      0.647      0.589      0.462\n",
      "Speed: 0.2ms preprocess, 22.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train1027\u001b[0m\n",
      "Overall mAP@0.5:0.95: 0.37579638959552963\n",
      "Overall Precision: 0.5099513560190709\n",
      "Overall Recall: 0.5770274341099696\n",
      "\n",
      "Per-class mAP:\n",
      "                class  mAP@0.5:0.95\n",
      "0          aeroplane      0.265253\n",
      "1            bicycle      0.403015\n",
      "2               bird      0.246676\n",
      "3               boat      0.342376\n",
      "4             bottle      0.194727\n",
      "5                bus      0.619195\n",
      "6                car      0.416351\n",
      "7                cat      0.573858\n",
      "8              chair      0.169137\n",
      "9                cow      0.333837\n",
      "10       diningtable      0.326941\n",
      "11               dog      0.534940\n",
      "12             horse      0.306032\n",
      "13         motorbike      0.435810\n",
      "14            person      0.463227\n",
      "15       pottedplant      0.147312\n",
      "16             sheep      0.349650\n",
      "17              sofa      0.282773\n",
      "18             train      0.642709\n",
      "19         tvmonitor      0.462108\n",
      "Overall      Overall      0.375796\n",
      "\n",
      "Per-class Precision:\n",
      "                class  Precision\n",
      "0          aeroplane   0.595060\n",
      "1            bicycle   0.473318\n",
      "2               bird   0.533033\n",
      "3               boat   0.478142\n",
      "4             bottle   0.441393\n",
      "5                bus   0.727787\n",
      "6                car   0.699448\n",
      "7                cat   0.706953\n",
      "8              chair   0.328578\n",
      "9                cow   0.501856\n",
      "10       diningtable   0.277014\n",
      "11               dog   0.694262\n",
      "12             horse   0.443706\n",
      "13         motorbike   0.323606\n",
      "14            person   0.786179\n",
      "15       pottedplant   0.366340\n",
      "16             sheep   0.471710\n",
      "17              sofa   0.237071\n",
      "18             train   0.587463\n",
      "19         tvmonitor   0.526110\n",
      "Overall      Overall   0.509951\n",
      "\n",
      "Per-class Recall:\n",
      "                class    Recall\n",
      "0          aeroplane  0.317982\n",
      "1            bicycle  0.642857\n",
      "2               bird  0.367089\n",
      "3               boat  0.758621\n",
      "4             bottle  0.486486\n",
      "5                bus  0.783784\n",
      "6                car  0.510204\n",
      "7                cat  0.787819\n",
      "8              chair  0.392308\n",
      "9                cow  0.559816\n",
      "10       diningtable  0.708333\n",
      "11               dog  0.597620\n",
      "12             horse  0.325581\n",
      "13         motorbike  0.666667\n",
      "14            person  0.517970\n",
      "15       pottedplant  0.301370\n",
      "16             sheep  0.592593\n",
      "17              sofa  0.687500\n",
      "18             train  0.888889\n",
      "19         tvmonitor  0.647059\n",
      "Overall      Overall  0.577027\n",
      "\n",
      "Per-class F1:\n",
      "           class        F1\n",
      "0     aeroplane  0.414479\n",
      "1       bicycle  0.545211\n",
      "2          bird  0.434764\n",
      "3          boat  0.586577\n",
      "4        bottle  0.462844\n",
      "5           bus  0.754748\n",
      "6           car  0.590022\n",
      "7           cat  0.745198\n",
      "8         chair  0.357625\n",
      "9           cow  0.529253\n",
      "10  diningtable  0.398272\n",
      "11          dog  0.642326\n",
      "12        horse  0.375574\n",
      "13    motorbike  0.435712\n",
      "14       person  0.624495\n",
      "15  pottedplant  0.330693\n",
      "16        sheep  0.525286\n",
      "17         sofa  0.352566\n",
      "18        train  0.707404\n",
      "19    tvmonitor  0.580350\n",
      "20      Overall  0.541419\n",
      "All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# 5. Compute full validation metrics\n",
    "# --------------------------\n",
    "metrics = model.val(\n",
    "    data=subset_yaml_path,\n",
    "    split=\"val\",\n",
    "    imgsz=320,\n",
    "    conf=0.1\n",
    ")\n",
    "\n",
    "print(\"Overall mAP@0.5:0.95:\", metrics.box.map)\n",
    "print(\"Overall Precision:\", metrics.box.mp)\n",
    "print(\"Overall Recall:\", metrics.box.mr)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 6. Export per-class metrics into DataFrames\n",
    "# --------------------------\n",
    "import pandas as pd\n",
    "\n",
    "# mAP DataFrame\n",
    "df_map = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"mAP@0.5:0.95\": metrics.box.maps\n",
    "})\n",
    "df_map.loc[\"Overall\"] = [\"Overall\", metrics.box.map]\n",
    "\n",
    "# Precision DataFrame\n",
    "df_precision = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Precision\": metrics.box.p\n",
    "})\n",
    "df_precision.loc[\"Overall\"] = [\"Overall\", metrics.box.mp]\n",
    "\n",
    "# Recall DataFrame\n",
    "df_recall = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"Recall\": metrics.box.r\n",
    "})\n",
    "df_recall.loc[\"Overall\"] = [\"Overall\", metrics.box.mr]\n",
    "\n",
    "# F1 DataFrame\n",
    "f1_per_class = 2 * (metrics.box.p * metrics.box.r) / (metrics.box.p + metrics.box.r + 1e-6)\n",
    "overall_f1 = 2 * (metrics.box.mp * metrics.box.mr) / (metrics.box.mp + metrics.box.mr + 1e-6)\n",
    "df_f1 = pd.DataFrame({\n",
    "    \"class\": VOC_CLASSES,\n",
    "    \"F1\": f1_per_class\n",
    "})\n",
    "df_f1.loc[len(df_f1)] = [\"Overall\", overall_f1]\n",
    "\n",
    "print(\"\\nPer-class mAP:\\n\", df_map)\n",
    "print(\"\\nPer-class Precision:\\n\", df_precision)\n",
    "print(\"\\nPer-class Recall:\\n\", df_recall)\n",
    "print(\"\\nPer-class F1:\\n\", df_f1)\n",
    "\n",
    "# Save to CSVs\n",
    "df_map.to_csv(os.path.join(repo_root, \"yolo_val_map.csv\"), index=False)\n",
    "df_precision.to_csv(os.path.join(repo_root, \"yolo_val_precision.csv\"), index=False)\n",
    "df_recall.to_csv(os.path.join(repo_root, \"yolo_val_recall.csv\"), index=False)\n",
    "df_f1.to_csv(os.path.join(repo_root, \"yolo_val_f1.csv\"), index=False)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Generate per-class bar charts\n",
    "# --------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# mAP bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_map[\"class\"][:-1], df_map[\"mAP@0.5:0.95\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"mAP@0.5:0.95\")\n",
    "plt.title(\"YOLOv8 Per-Class mAP (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_map.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Precision bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_precision[\"class\"][:-1], df_precision[\"Precision\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"YOLOv8 Per-Class Precision (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_precision.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Recall bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_recall[\"class\"][:-1], df_recall[\"Recall\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"YOLOv8 Per-Class Recall (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_recall.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# F1 bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_f1[\"class\"][:-1], df_f1[\"F1\"][:-1])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"YOLOv8 Per-Class F1 (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_f1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# --------------------------\n",
    "# 8. Combined grouped bar chart (Precision, Recall, F1)\n",
    "# --------------------------\n",
    "classes = df_precision[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))  # positions\n",
    "width = 0.25\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width, df_precision[\"Precision\"][:-1], width=width, label=\"Precision\")\n",
    "plt.bar(x, df_recall[\"Recall\"][:-1], width=width, label=\"Recall\")\n",
    "plt.bar(x + width, df_f1[\"F1\"][:-1], width=width, label=\"F1\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Precision, Recall, and F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_prf1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"All metrics exported: mAP, Precision, Recall, F1 (CSV + individual + grouped bar charts).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ba92ed6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Scatterplot: Per-class mAP vs F1\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pearsonr\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Exclude \"Overall\" row\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Scatterplot: Per-class mAP vs F1\n",
    "# --------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Exclude \"Overall\" row\n",
    "perclass_map = df_map[\"mAP@0.5:0.95\"][:-1]\n",
    "perclass_f1 = df_f1[\"F1\"][:-1]\n",
    "classes = df_map[\"class\"][:-1]\n",
    "\n",
    "# Compute correlation\n",
    "corr, pval = pearsonr(perclass_map, perclass_f1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=perclass_map, y=perclass_f1)\n",
    "\n",
    "# Annotate each class\n",
    "for i, cls in enumerate(classes):\n",
    "    plt.text(perclass_map.iloc[i] + 0.005,\n",
    "             perclass_f1.iloc[i] + 0.005,\n",
    "             cls, fontsize=8)\n",
    "\n",
    "plt.xlabel(\"mAP@0.5:0.95\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(f\"YOLOv8 Per-Class mAP vs F1 (Validation)\\nPearson r={corr:.2f}, p={pval:.3f}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_map_vs_f1_scatter.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated scatterplot: Per-class mAP vs F1 (with correlation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f972399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated grouped bar chart: Per-class mAP vs F1\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Compare per-class mAP vs F1\n",
    "# --------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "classes = df_map[\"class\"][:-1]  # exclude \"Overall\"\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35  # bar width\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(x - width/2, df_map[\"mAP@0.5:0.95\"][:-1], width=width, label=\"mAP@0.5:0.95\")\n",
    "plt.bar(x + width/2, df_f1[\"F1\"][:-1], width=width, label=\"F1 Score\")\n",
    "\n",
    "plt.xticks(x, classes, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"YOLOv8 Per-Class Comparison: mAP vs F1 (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig(os.path.join(repo_root, \"yolo_perclass_map_vs_f1.png\"), bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Generated grouped bar chart: Per-class mAP vs F1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4910bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# 8. Side-by-side GT vs Predictions (10 random val images)\n",
    "# --------------------------\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "val_images_dir = os.path.join(subset_dir, \"val\", \"images\")\n",
    "val_labels_dir = os.path.join(subset_dir, \"val\", \"labels\")\n",
    "\n",
    "# Randomly select 10 val images\n",
    "all_val_images = glob.glob(os.path.join(val_images_dir, \"*.jpg\"))\n",
    "sample_val_images = random.sample(all_val_images, min(10, len(all_val_images)))\n",
    "\n",
    "comparison_dir = os.path.join(repo_root, \"comparison_predictions\")\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "\n",
    "for idx, img_path in enumerate(sample_val_images, start=1):\n",
    "    # Load GT labels\n",
    "    label_path = os.path.join(val_labels_dir, os.path.basename(img_path).replace(\".jpg\", \".txt\"))\n",
    "    gt_boxes = []\n",
    "    if os.path.exists(label_path):\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                cls_id, x, y, w, h = line.strip().split()\n",
    "                cls_id = int(cls_id)\n",
    "                gt_boxes.append((VOC_CLASSES[cls_id], float(x), float(y), float(w), float(h)))\n",
    "\n",
    "    # Run YOLO prediction\n",
    "    results = model.predict(source=img_path, imgsz=320, conf=0.1, verbose=False)\n",
    "    res = results[0]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "    # Left: Ground truth (draw bounding boxes manually)\n",
    "    img = mpimg.imread(img_path)\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    h, w = img.shape[:2]\n",
    "    for cls_name, x, y, bw, bh in gt_boxes:\n",
    "        xmin = int((x - bw/2) * w)\n",
    "        ymin = int((y - bh/2) * h)\n",
    "        xmax = int((x + bw/2) * w)\n",
    "        ymax = int((y + bh/2) * h)\n",
    "        rect = plt.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin,\n",
    "                             linewidth=2, edgecolor=\"lime\", facecolor=\"none\")\n",
    "        axes[0].add_patch(rect)\n",
    "        axes[0].text(xmin, ymin-5, cls_name, color=\"lime\", fontsize=10, weight=\"bold\")\n",
    "\n",
    "    # Right: YOLO predictions (ultralytics res.plot())\n",
    "    img_pred = res.plot()\n",
    "    axes[1].imshow(img_pred)\n",
    "    axes[1].set_title(\"YOLO Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(comparison_dir, f\"comparison_{idx:02d}.png\")\n",
    "    fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Saved GT vs Prediction comparison: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pascalvoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
